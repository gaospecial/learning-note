[{"path":"index.html","id":"前言","chapter":"前言","heading":"前言","text":"","code":""},{"path":"rmarkdown.html","id":"rmarkdown","chapter":"第 1 章 R Markdown","heading":"第 1 章 R Markdown","text":"","code":""},{"path":"rmarkdown.html","id":"设置选项钩子附上图片下载链接","chapter":"第 1 章 R Markdown","heading":"1.1 设置选项钩子附上图片下载链接","text":"通过设置选项钩子，可以在图片输出设备多于 1 个时候，提供图片的下载链接。下图中，使用了 dev = c('png', 'pdf', \"tiff\", \"svg\") 等 4 种图像设备，由于 dev 选项钩子的存在，将自动在图注中添加图片的下载链接。\n图 1.1: plot. png | pdf | tiff | svg\n图注也可以使用文本引用，不影响钩子的应用效果。\n图 1.2: 使用选项钩子在图注中添加下载链接。 png | pdf\n","code":"\nknitr::opts_hooks$set(dev = function(options){\n  if (length(options$dev) > 1){\n    x = paste0(\"[\", options$dev, \"](\", knitr::fig_path(options$dev), \")\",\n               collapse = \" | \")\n    options$fig.cap = paste(options$fig.cap, x, sep = \" \")\n  }\n  options\n})\nplot(1:10)\nplot(1:10)"},{"path":"rmarkdown.html","id":"使用自定义图形设备","chapter":"第 1 章 R Markdown","heading":"1.2 使用自定义图形设备","text":"possible devices supported knitr : “bmp”, “postscript”, “pdf”, “png”, “svg”, “jpeg”, “pictex”, “tiff”, “win.metafile”, “cairo_pdf”, “cairo_ps”, “quartz_pdf”, “quartz_png”, “quartz_jpeg”, “quartz_tiff”, “quartz_gif”, “quartz_psd”, “quartz_bmp”, “CairoJPEG”, “CairoPNG”, “CairoPS”, “CairoPDF”, “CairoSVG”, “CairoTIFF”, “Cairo_pdf”, “Cairo_png”, “Cairo_ps”, “Cairo_svg”, “svglite”, “ragg_png”, “tikz”.: https://bookdown.org/yihui/rmarkdown-cookbook/graphical-device.html是不是可以使用 ppt 设备呢？dev option can also name function. , function seems like function(file, width, height), described https://yihui.org/knitr/options/#plots.Therefore, new PPTX device can defined :Notably, multiple devices used, may need specify fig.ext option. example, dev = c('png', 'pdf', 'pptx'), fig.ext=c(\"png\",\"pdf\",\"pptx\"). See https://yihui.org/knitr/demo/graphics/#-note--custom-graphical-devices.\n图 1.3: 使用选项钩子在图注中添加下载链接。 png | pdf | pptx\n","code":"\nlibrary(export)\n# a pptx device\npptx = function(file, width, height){\n  export::graph2ppt(file = file, width = width, height = height)\n}\nplot(1:10)"},{"path":"using-vegan.html","id":"using-vegan","chapter":"第 2 章 Vegan 使用技巧","heading":"第 2 章 Vegan 使用技巧","text":"R package community ecologists: popular ordination methods, ecological null models & diversity analysisVegan 是面向研究群落的生态学家开发的软件包，包括了流行的坐标分析方法、生态学的零假设和多样性分析方法。","code":"\nlibrary(vegan)"},{"path":"using-vegan.html","id":"计算-beta-dispersion","chapter":"第 2 章 Vegan 使用技巧","heading":"2.1 计算 beta-dispersion","text":"使用 betadisper() 计算。这是一个 beta-diversity （β多样性）指标。","code":"\nexample(\"betadisper\")"},{"path":"using-vegan.html","id":"使用-varespec-数据集计算-beta-dispersion","chapter":"第 2 章 Vegan 使用技巧","heading":"2.1.1 使用 varespec 数据集计算 beta-dispersion","text":"","code":""},{"path":"using-vegan.html","id":"数据集说明","chapter":"第 2 章 Vegan 使用技巧","heading":"2.1.1.1 数据集说明","text":"varespec 数据有 24 行和 44 列，是 44 个物种（species）在 24 个样品（site）中的估计覆盖率，\n列名是 44 个物种拉丁名称的简写。varechem 数据有 24 行、14 列，是 24 个样品\n中 14 个土壤相关的属性。","code":"\ndata(varespec)\n\n## Bray-Curtis distances between samples\ndis <- vegdist(varespec)"},{"path":"using-vegan.html","id":"样本间距","chapter":"第 2 章 Vegan 使用技巧","heading":"2.1.1.2 样本间距","text":"vegdist 默认计算样品间的 Bray-Curtis 距离，其定义为：\\[d_{jk} = (\\sum abs(x_{ij}-x_{ik}))/(\\sum (x_{ij}+x_{ik}))\\]其中，\\(x_{ij}\\) 和 \\(x_{ik}\\) 分别代表物种（列） \\(\\) 在两个样品（行）\\(j\\) 和 \\(k\\) 中的数量。\n则前两个位点的 Bray-Curtis 距离是 0.5310021。","code":""},{"path":"using-vegan.html","id":"样本分组及计算结果","chapter":"第 2 章 Vegan 使用技巧","heading":"2.1.1.3 样本分组及计算结果","text":"mod 返回一个 betadisper 类的列表，有以下几个元件：eig：numeric；PCoA 分析的特征值eig：numeric；PCoA 分析的特征值vectors: matrix；PCoA 分析特征向量vectors: matrix；PCoA 分析特征向量distances: numeric；在 PCoA 的多维空间中，每个样本与其对应的分组中心间的欧氏距离。distances: numeric；在 PCoA 的多维空间中，每个样本与其对应的分组中心间的欧氏距离。group: factor；分组信息。group: factor；分组信息。centroids: matrix；分组在 PCoA 坐标系中的中心点。centroids: matrix；分组在 PCoA 坐标系中的中心点。call: 函数调用信息。call: 函数调用信息。这 24 个样品来自两组处理，其中前 16 个为一组（grazed），后 8 个为另一组（ungrazed）。\nbetadisper() 计算了每个组样品距离中心点（“中位数”）距离的平均值。输出中还包括\n23 个特征值（eigenvalues）。","code":"\n## First 16 sites grazed, remaining 8 sites ungrazed\ngroups <- factor(c(rep(1,16), rep(2,8)), labels = c(\"grazed\",\"ungrazed\"))\n\n## Calculate multivariate dispersions\nmod <- betadisper(dis, groups)\nmod## \n##  Homogeneity of multivariate dispersions\n## \n## Call: betadisper(d = dis, group = groups)\n## \n## No. of Positive Eigenvalues: 15\n## No. of Negative Eigenvalues: 8\n## \n## Average distance to median:\n##   grazed ungrazed \n##   0.3926   0.2706 \n## \n## Eigenvalues for PCoA axes:\n## (Showing 8 of 23 eigenvalues)\n##  PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n## 1.7552 1.1334 0.4429 0.3698 0.2454 0.1961 0.1751 0.1284\nstr(mod)## List of 7\n##  $ eig            : Named num [1:23] 1.755 1.133 0.443 0.37 0.245 ...\n##   ..- attr(*, \"names\")= chr [1:23] \"PCoA1\" \"PCoA2\" \"PCoA3\" \"PCoA4\" ...\n##  $ vectors        : num [1:24, 1:23] 0.0946 -0.3125 -0.3511 -0.3291 -0.1926 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:24] \"18\" \"15\" \"24\" \"27\" ...\n##   .. ..$ : chr [1:23] \"PCoA1\" \"PCoA2\" \"PCoA3\" \"PCoA4\" ...\n##  $ distances      : Named num [1:24] 0.331 0.245 0.409 0.406 0.271 ...\n##   ..- attr(*, \"names\")= chr [1:24] \"18\" \"15\" \"24\" \"27\" ...\n##  $ group          : Factor w/ 2 levels \"grazed\",\"ungrazed\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ centroids      : num [1:2, 1:23] -0.1455 0.2786 0.0758 -0.2111 -0.0137 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"grazed\" \"ungrazed\"\n##   .. ..$ : chr [1:23] \"PCoA1\" \"PCoA2\" \"PCoA3\" \"PCoA4\" ...\n##  $ group.distances: num [1:2(1d)] 0.393 0.271\n##   ..- attr(*, \"dimnames\")=List of 1\n##   .. ..$ : chr [1:2] \"grazed\" \"ungrazed\"\n##  $ call           : language betadisper(d = dis, group = groups)\n##  - attr(*, \"class\")= chr \"betadisper\"\n##  - attr(*, \"method\")= chr \"bray\"\n##  - attr(*, \"type\")= chr \"median\"\n##  - attr(*, \"bias.adjust\")= logi FALSE"},{"path":"using-vegan.html","id":"统计分析","chapter":"第 2 章 Vegan 使用技巧","heading":"2.1.1.4 统计分析","text":"test one groups variable others, ANOVA distances group centroids can performed parametric theory used interpret significance F. alternative use permutation test. permutest.betadisper permutes model residuals generate permutation distribution F Null hypothesis difference dispersion groups.anova() 方法被用来比较各个分组间是否存在有变化程度更大的情况。permutest() 则为这一任务提供了另一种统计分析手段。anova() 和 permutest() 分别计算显著性。TukeyHSD() 方法则计算出两两 group 之间存在的差异及其显著性。plot() 可以绘制样品和中心点之间的距离。scores() 计算样本特征值。计算给出的样本、样本组的中心点坐标等。bias.adjust = TRUE 对于 beta-多样性估计中的小样本偏差进行校正。哪怕只有一个分组，仍然可以使用当有缺失值的时候，也可以使用type = \"centroid\" 时的结果（默认为 type = \"median\"，一般不需要修改）。","code":"\n## Perform test\nanova(mod)## Analysis of Variance Table\n## \n## Response: Distances\n##           Df  Sum Sq  Mean Sq F value  Pr(>F)  \n## Groups     1 0.07931 0.079306  4.6156 0.04295 *\n## Residuals 22 0.37801 0.017182                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Permutation test for F\npermutest(mod, pairwise = TRUE, permutations = 99)## \n## Permutation test for homogeneity of multivariate dispersions\n## Permutation: free\n## Number of permutations: 99\n## \n## Response: Distances\n##           Df  Sum Sq  Mean Sq      F N.Perm Pr(>F)  \n## Groups     1 0.07931 0.079306 4.6156     99   0.04 *\n## Residuals 22 0.37801 0.017182                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Pairwise comparisons:\n## (Observed p-value below diagonal, permuted p-value above diagonal)\n##           grazed ungrazed\n## grazed               0.04\n## ungrazed 0.04295\n## Tukey's Honest Significant Differences\n(mod.HSD <- TukeyHSD(mod))##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = distances ~ group, data = df)\n## \n## $group\n##                       diff        lwr          upr     p adj\n## ungrazed-grazed -0.1219422 -0.2396552 -0.004229243 0.0429502\nplot(mod.HSD)\n## Plot the groups and distances to centroids on the\n## first two PCoA axes\nplot(mod)\n## with data ellipses instead of hulls\nplot(mod, ellipse = TRUE, hull = FALSE) # 1 sd data ellipse\nplot(mod, ellipse = TRUE, hull = FALSE, conf = 0.90) # 90% data ellipse\n## can also specify which axes to plot, ordering respected\nplot(mod, axes = c(3,1), seg.col = \"forestgreen\", seg.lty = \"dashed\")\n## Draw a boxplot of the distances to centroid for each group\nboxplot(mod)\n## `scores` and `eigenvals` also work\nscrs <- scores(mod)\nstr(scrs)## List of 2\n##  $ sites    : num [1:24, 1:2] 0.0946 -0.3125 -0.3511 -0.3291 -0.1926 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:24] \"18\" \"15\" \"24\" \"27\" ...\n##   .. ..$ : chr [1:2] \"PCoA1\" \"PCoA2\"\n##  $ centroids: num [1:2, 1:2] -0.1455 0.2786 0.0758 -0.2111\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"grazed\" \"ungrazed\"\n##   .. ..$ : chr [1:2] \"PCoA1\" \"PCoA2\"\n# 可以将其画在图上\nplot(scrs$sites, xlim = c(-.8, .8), ylim = c(-.5, .5))\ntext(scrs$sites, labels = rownames(scrs$sites), pos = 4)\nhead(scores(mod, 1:4, display = \"sites\"))##          PCoA1       PCoA2        PCoA3        PCoA4\n## 18  0.09459373  0.15914576  0.074400844 -0.202466025\n## 15 -0.31248809  0.10032751 -0.062243360  0.110844864\n## 24 -0.35106507 -0.05954096 -0.038079447  0.095060928\n## 27 -0.32914546 -0.17019348  0.231623720  0.019110623\n## 23 -0.19259443 -0.01459250 -0.005679372 -0.209718312\n## 19 -0.06794575 -0.14501690 -0.085645653  0.002431355\n# group centroids/medians \nscores(mod, 1:4, display = \"centroids\")##               PCoA1       PCoA2       PCoA3      PCoA4\n## grazed   -0.1455200  0.07584572 -0.01366220 -0.0178990\n## ungrazed  0.2786095 -0.21114993 -0.03475586  0.0220129\n# eigenvalues from the underlying principal coordinates analysis\neigenvals(mod)##      PCoA1      PCoA2      PCoA3      PCoA4      PCoA5      PCoA6      PCoA7 \n##  1.7552165  1.1334455  0.4429018  0.3698054  0.2453532  0.1960921  0.1751131 \n##      PCoA8      PCoA9     PCoA10     PCoA11     PCoA12     PCoA13     PCoA14 \n##  0.1284467  0.0971594  0.0759601  0.0637178  0.0583225  0.0394934  0.0172699 \n##     PCoA15     PCoA16     PCoA17     PCoA18     PCoA19     PCoA20     PCoA21 \n##  0.0051011 -0.0004131 -0.0064654 -0.0133147 -0.0253944 -0.0375105 -0.0480069 \n##     PCoA22     PCoA23 \n## -0.0537146 -0.0741390\n## try out bias correction; compare with mod3\n(mod3B <- betadisper(dis, groups, type = \"median\", bias.adjust=TRUE))## \n##  Homogeneity of multivariate dispersions\n## \n## Call: betadisper(d = dis, group = groups, type = \"median\", bias.adjust\n## = TRUE)\n## \n## No. of Positive Eigenvalues: 15\n## No. of Negative Eigenvalues: 8\n## \n## Average distance to median:\n##   grazed ungrazed \n##   0.4055   0.2893 \n## \n## Eigenvalues for PCoA axes:\n## (Showing 8 of 23 eigenvalues)\n##  PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n## 1.7552 1.1334 0.4429 0.3698 0.2454 0.1961 0.1751 0.1284\nanova(mod3B)## Analysis of Variance Table\n## \n## Response: Distances\n##           Df  Sum Sq  Mean Sq F value  Pr(>F)  \n## Groups     1 0.07193 0.071927  3.7826 0.06468 .\n## Residuals 22 0.41834 0.019015                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npermutest(mod3B, permutations = 99)## \n## Permutation test for homogeneity of multivariate dispersions\n## Permutation: free\n## Number of permutations: 99\n## \n## Response: Distances\n##           Df  Sum Sq  Mean Sq      F N.Perm Pr(>F)  \n## Groups     1 0.07193 0.071927 3.7826     99   0.08 .\n## Residuals 22 0.41834 0.019015                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## should always work for a single group\ngroup <- factor(rep(\"grazed\", NROW(varespec)))\n(tmp <- betadisper(dis, group, type = \"median\"))## \n##  Homogeneity of multivariate dispersions\n## \n## Call: betadisper(d = dis, group = group, type = \"median\")\n## \n## No. of Positive Eigenvalues: 15\n## No. of Negative Eigenvalues: 8\n## \n## Average distance to median:\n## grazed \n## 0.4255 \n## \n## Eigenvalues for PCoA axes:\n## (Showing 8 of 23 eigenvalues)\n##  PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n## 1.7552 1.1334 0.4429 0.3698 0.2454 0.1961 0.1751 0.1284\n(tmp <- betadisper(dis, group, type = \"centroid\"))## \n##  Homogeneity of multivariate dispersions\n## \n## Call: betadisper(d = dis, group = group, type = \"centroid\")\n## \n## No. of Positive Eigenvalues: 15\n## No. of Negative Eigenvalues: 8\n## \n## Average distance to centroid:\n## grazed \n## 0.4261 \n## \n## Eigenvalues for PCoA axes:\n## (Showing 8 of 23 eigenvalues)\n##  PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n## 1.7552 1.1334 0.4429 0.3698 0.2454 0.1961 0.1751 0.1284\n## simulate missing values in 'd' and 'group'\n## using spatial medians\ngroups[c(2,20)] <- NA\ndis[c(2, 20)] <- NA\nmod2 <- betadisper(dis, groups) ## messages\nmod2## \n##  Homogeneity of multivariate dispersions\n## \n## Call: betadisper(d = dis, group = groups)\n## \n## No. of Positive Eigenvalues: 14\n## No. of Negative Eigenvalues: 5\n## \n## Average distance to median:\n##   grazed ungrazed \n##   0.3984   0.3008 \n## \n## Eigenvalues for PCoA axes:\n## (Showing 8 of 19 eigenvalues)\n##  PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n## 1.4755 0.8245 0.4218 0.3456 0.2159 0.1688 0.1150 0.1060\npermutest(mod2, permutations = 99)## \n## Permutation test for homogeneity of multivariate dispersions\n## Permutation: free\n## Number of permutations: 99\n## \n## Response: Distances\n##           Df   Sum Sq  Mean Sq      F N.Perm Pr(>F)\n## Groups     1 0.039979 0.039979 2.4237     99   0.11\n## Residuals 18 0.296910 0.016495\nanova(mod2)## Analysis of Variance Table\n## \n## Response: Distances\n##           Df   Sum Sq  Mean Sq F value Pr(>F)\n## Groups     1 0.039979 0.039979  2.4237 0.1369\n## Residuals 18 0.296910 0.016495\nplot(mod2)\nboxplot(mod2)\nplot(TukeyHSD(mod2))\n## Using group centroids\nmod3 <- betadisper(dis, groups, type = \"centroid\")\nmod3## \n##  Homogeneity of multivariate dispersions\n## \n## Call: betadisper(d = dis, group = groups, type = \"centroid\")\n## \n## No. of Positive Eigenvalues: 14\n## No. of Negative Eigenvalues: 5\n## \n## Average distance to centroid:\n##   grazed ungrazed \n##   0.4001   0.3108 \n## \n## Eigenvalues for PCoA axes:\n## (Showing 8 of 19 eigenvalues)\n##  PCoA1  PCoA2  PCoA3  PCoA4  PCoA5  PCoA6  PCoA7  PCoA8 \n## 1.4755 0.8245 0.4218 0.3456 0.2159 0.1688 0.1150 0.1060\npermutest(mod3, permutations = 99)## \n## Permutation test for homogeneity of multivariate dispersions\n## Permutation: free\n## Number of permutations: 99\n## \n## Response: Distances\n##           Df   Sum Sq  Mean Sq      F N.Perm Pr(>F)\n## Groups     1 0.033468 0.033468 3.1749     99    0.1\n## Residuals 18 0.189749 0.010542\nanova(mod3)## Analysis of Variance Table\n## \n## Response: Distances\n##           Df   Sum Sq  Mean Sq F value  Pr(>F)  \n## Groups     1 0.033468 0.033468  3.1749 0.09166 .\n## Residuals 18 0.189749 0.010542                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nplot(mod3)\nboxplot(mod3)\nplot(TukeyHSD(mod3))"},{"path":"using-vegan.html","id":"使用-dune-数据集计算-beta-dispersion","chapter":"第 2 章 Vegan 使用技巧","heading":"2.1.2 使用 dune 数据集计算 beta-dispersion","text":"本例子来源于 Vignettes: Diversity analysis vegan.使用的数据是荷兰沙丘（dune）草地牧场（meadow）上的植被（vegetation）和环境因子。使用 betadiver() 计算 beta-多样性。betadiver() 的第二个参数指定计算 beta 多样性指标的算法。算法基于 \\(\\), \\(b\\), \\(c\\) 等 3 个值（使用 betadiver(help = TRUE) 查看计算公式）。它们的含义如下：method = \"z\" 时，计算的公式为：\\[z = (log(2) - log(2*+b+c) + log(+b+c))/log(2)\\]计算得到的 z 是一个 dist 对象。按照 dune.env$Management 中给出的土地管理类型进行分组，计算 beta-dispersion。对计算得到的结果进行可视化。","code":"\n# Vegetation and Environment in Dutch Dune Meadows.\ndata(\"dune\")\ndata(\"dune.env\")\nz = betadiver(dune, method = \"z\")\nz##            1         2         3         4         5         6         7\n## 2  0.4150375                                                            \n##            8         9        10        11        12        13        14\n## 2                                                                       \n##           15        16        17        18        19\n## 2                                                   \n##  [ reached getOption(\"max.print\") -- omitted 18 rows ]\n(mod = with(dune.env, betadisper(z, Management)))## \n##  Homogeneity of multivariate dispersions\n## \n## Call: betadisper(d = z, group = Management)\n## \n## No. of Positive Eigenvalues: 12\n## No. of Negative Eigenvalues: 7\n## \n## Average distance to median:\n##     BF     HF     NM     SF \n## 0.2532 0.2512 0.4406 0.3635 \n## \n## Eigenvalues for PCoA axes:\n## (Showing 8 of 19 eigenvalues)\n##   PCoA1   PCoA2   PCoA3   PCoA4   PCoA5   PCoA6   PCoA7   PCoA8 \n## 1.65466 0.88696 0.53336 0.37435 0.28725 0.22445 0.16128 0.08099\nplot(mod)\nboxplot(mod)"},{"path":"probability-statistics.html","id":"probability-statistics","chapter":"第 3 章 概率论和数理统计","heading":"第 3 章 概率论和数理统计","text":"http://staff.ustc.edu.cn/~zwp/teach/Prob-Stat/probstat.htm","code":"\n# install.packages(\"TeachingDemos\")\nlibrary(\"TeachingDemos\")"},{"path":"probability-statistics.html","id":"roll-the-dice","chapter":"第 3 章 概率论和数理统计","heading":"3.1 掷骰子","text":"","code":"\ndice(10, ndice=2, plot.it=T)"},{"path":"probability-statistics.html","id":"efrons-dice","chapter":"第 3 章 概率论和数理统计","heading":"3.1.1 Efron’s dice","text":"see: http://mathworld.wolfram.com/EfronsDice.html。","code":"\n ed <- list( rep( c(4,0), c(4,2) ),\n rep(3,6), rep( c(6,2), c(2,4) ),\n rep( c(5,1), c(3,3) ) )\n     \n tmp <- dice( 10000, ndice=4 )\n dice(10,ndice=4,plot.it=T)\n ed.out <- sapply(1:4, function(i) ed[[i]][ tmp[[i]] ] )\n     \n mean(ed.out[,1] > ed.out[,2])## [1] 0.6666\n mean(ed.out[,2] > ed.out[,3])## [1] 0.6663\n mean(ed.out[,3] > ed.out[,4])## [1] 0.6663\n mean(ed.out[,4] > ed.out[,1])## [1] 0.6603"},{"path":"probability-statistics.html","id":"buffons-needle","chapter":"第 3 章 概率论和数理统计","heading":"3.2 Buffon’s needle","text":"See: https://yihui.org/animation/example/buffon-needle/","code":"\n# install.packages(\"animation\")\nlibrary(animation)\noopt = ani.options(nmax = 5, interval = 0)\nopar = par(mar = c(3, 2.5, 0.5, 0.2), pch = 20, mgp = c(1.5, 0.5, 0))\nbuffon.needle()"},{"path":"probability-statistics.html","id":"sample---sampling-from-a-finite-set","chapter":"第 3 章 概率论和数理统计","heading":"3.3 sample() - sampling from a finite set","text":"Simulate dice throws loaded die.","code":"\n# sample(x, size, replace = FALSE, prob = NULL)\n\n# Lotto genrator (sampling from 1:39 without replacement)\n\nsample(39, 7) # or: sample(1:39, 7)## [1] 33 39 13 37 27 12 20\n# Ten rows of LOTTO (surely one of these must win?!?)\n\nreplicate(10, sample(39, 7))##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]   36   17   18   15   31   38   34   34   29    26\n## [2,]   14   23   35   12    2    9   32   35   23    28\n## [3,]   13   11    2   22   18    1   25   39   34    24\n##  [ reached getOption(\"max.print\") -- omitted 4 rows ]\n# Use the argument prob, if you need to specify different\n# probabilities for the different outcomes.\n\n# Sometimes we want sampling with replacement; this is the\n# same as drawing an i.i.d. sequence of values from the\n# corresponding discrete distribution.\n\n# Simulate dice throws\n\nfair.die <- sample(1:6, 100, replace = TRUE)\ntable(fair.die)## fair.die\n##  1  2  3  4  5  6 \n## 22 18 12 17 16 15\nloaded.die <- sample(1:6, 100, replace = TRUE, prob = c(2, 2, 2, 2, 2, 5))\ntable(loaded.die)## loaded.die\n##  1  2  3  4  5  6 \n## 18 13 13 22  8 26"},{"path":"probability-statistics.html","id":"discrete-random-variables-and-their-distributions","chapter":"第 3 章 概率论和数理统计","heading":"3.4 Discrete random variables and their distributions","text":"BinomialThe Hypergeometric DistributionThe Geometric Distribution,let X count number failures first successesThe Negative Binomial Distribution, let X count number failures r successesPoisson distributinoPoisson normal approximation binomial probabilities, estimated parametersBinomialThe Hypergeometric DistributionThe Geometric Distribution,let X count number failures first successes4.Negative Binomial Distribution, let X count number failures r successesPoisson distributinoNormal distributionsome plotsexponential distributionUniform distributionother distributionhttp://zoonek2.free.fr/UNIX/48_R/07.html","code":"\ndbinom(x=2,size=20,prob=0.5)## [1] 0.0001811981\npbinom(q=2,size=20,prob=0.5)## [1] 0.0002012253\nqbinom(p=0.4,size=20,prob=0.5)## [1] 9\nrbinom(n=5,size=20,prob=0.5)## [1]  5 13 10 10 12\nplot(dbinom(0:20,size=20,prob=0.5),type=\"h\")\nplot(dbinom(0:20,size=20,prob=0.8),type=\"h\")\ndhyper(x=2, m=10, n=30, k=6)## [1] 0.3212879\nphyper(q=2, m=10, n=30, k=6)## [1] 0.8472481\nqhyper(0.3, m=10, n=30, k=6)## [1] 1\nrhyper(nn=10, m=10, n=30, k=6)##  [1] 2 0 3 1 2 3 0 3 3 2\ndgeom(4,prob=0.8)## [1] 0.00128\npgeom(4, prob = 0.8)## [1] 0.99968\nqgeom(0.4,prob=0.8)## [1] 0\nrgeom(10,prob=0.8)##  [1] 0 0 2 0 0 0 0 0 0 0\nplot(dgeom(0:20,prob=0.5),type=\"h\")\nplot(dgeom(0:20,prob=0.8),type=\"h\")\ndnbinom(x=5,size=3,prob=0.4)   ## [1] 0.1045094\npnbinom(5,size=3,prob=0.4)## [1] 0.6846054\nqnbinom(0.5,size=3,prob=0.4)## [1] 4\nrnbinom(n=10,size=3,prob=0.4)##  [1]  2  7 11  5  3  5  8  7  5  2\nplot(dnbinom(0:20,size=5,p=0.5),type=\"h\")\ndpois(x=0,lambda=2.4)## [1] 0.09071795\nppois(q=10,lambda=2.4)## [1] 0.999957\nqpois(p=0.9,lambda=2.4)## [1] 4\nrpois(n=10,lambda=2.4)##  [1] 4 3 2 3 1 0 1 0 2 2\nplot(dpois(0:20,lambda=1),type=\"h\")\nx <- 0:20\nplot(x, ppois(x, 1), type=\"s\", lty=1,ylab=\"F(x)\", main=\"Poisson approx of binomial\")\nlines(x, pbinom(x, 100, 0.01),type=\"s\",col=2,lty=2)\nlegend(\"bottomright\",legend=c(\"Poisson\",\"Binomial\"),lty=1:2,col=1:2)\n#P(X<=k)=pbinom(k,n,p)\n#Poisson approximation: P(X<=k) app ppois(k,np)\n#Normal approximation: P(X<=k) app pnorm(k,np,npq)\n\n\napprx <- function(n, p, R = 1000, k = 6) {\n  trueval <- pbinom(k, n, p) # true binomial probability\n  prob.zcc <- prob.zncc <- prob.pois <- NULL  \n  q<-1-p\n  for (i in 1:R) {\n    x <- rnorm(n, n * p, sqrt(n * p * q))\n    z.cc <- ((k + .5) - mean(x))/sd(x) # with cont. correction\n    prob.zcc[i] <- pnorm(z.cc)\n    z.ncc <- (k - mean(x))/sd(x) # no cont. correction\n    prob.zncc[i] <- pnorm(z.ncc)    \n    y <- rpois(n, n * p)\n    prob.pois[i] <- length(y[y <= k])/n\n  }\n list(prob.zcc = prob.zcc, prob.zncc = prob.zncc, \n       prob.pois = prob.pois, trueval = trueval)\n}\nR <- 1000\nset.seed(10)\nout <- apprx(n = 200, p = .03, k = 6, R = 1000)\n# windows(6,5)\n plot(1:R, out$prob.pois, type = \"l\", col = \"green\", xlab = \"Runs\", \n      main = expression(paste(\"Simulated Probabilities: \", \n             n==200, \", \", p==0.03, sep=\"\")),\n      ylab = \"Probability\", ylim = c(.3, .7))\n abline(h = out$trueval, col=\"red\", lty=2)\n lines(1:R, out$prob.zcc, lty = 1, col = \"purple\")\n lines(1:R, out$prob.zncc, lty = 1, col = \"orange\")\n legend(\"bottomleft\", c(\"Poisson\", \"Normal (with cc)\", \n          \"Normal (w/o cc)\"),\n        lty = c(1), col = c(\"green\", \"purple\", \"orange\"))\nset.seed(10)\nout <- apprx(n = 200, p = .03, k = 6, R = 1000)\n# windows(6,5)\nboxplot(out$prob.pois, boxwex = 0.25, at = 1:1 - .25,\n        col = \"green\",\n        main = expression(paste(\"Approximating Binomial Probability: \", \n                                n==200, \", \", p==0.03, sep=\"\")),\n        ylab = \"Probablity\", \n        ylim = c(out$trueval - 0.2, out$trueval + 0.25))\nboxplot(out$prob.zcc, boxwex = 0.25, at = 1:1 + 0, add = T,\n         col = \"purple\")\nboxplot(out$prob.zncc, boxwex = 0.25, at = 1:1 + 0.25, add = T,\n         col = \"orange\" )\nabline(h = out$trueval, col = \"red\", lty=2)\nlegend(\"topleft\", c(\"Poisson\", \"Normal (with cc)\", \"Normal (w/o cc)\"), \n           fill = c(\"green\", \"purple\", \"orange\"))\ndbinom(x=2,size=10,prob=0.4)## [1] 0.1209324\npbinom(q=2,size=10,prob=0.4)## [1] 0.1672898\nqbinom(p=0.4,size=10,prob=0.4)## [1] 4\nrbinom(n=5,size=10,prob=0.4)## [1] 3 5 2 1 4\ndhyper(x=2, m=10, n=30, k=6)## [1] 0.3212879\n phyper(q=2, m=10, n=30, k=6)## [1] 0.8472481\n qhyper(0.3, m=10, n=30, k=6)## [1] 1\n rhyper(nn=10, m=10, n=30, k=6)##  [1] 4 1 1 1 2 2 1 1 3 2\ndgeom(4,prob=0.8)## [1] 0.00128\npgeom(4, prob = 0.8)## [1] 0.99968\nqgeom(0.4,prob=0.8)## [1] 0\nrgeom(10,prob=0.8)##  [1] 0 1 0 0 0 1 0 0 1 0\ndnbinom(x=5,size=3,prob=0.4)   ## [1] 0.1045094\npnbinom(5,size=3,prob=0.4)## [1] 0.6846054\nqnbinom(0.5,size=3,prob=0.4)## [1] 4\nrnbinom(n=10,size=3,prob=0.4)##  [1]  2  5  1  1 11  4  2  1  7 14\ndpois(x=0,lambda=2.4)## [1] 0.09071795\nppois(q=10,lambda=2.4)## [1] 0.999957\nqpois(p=0.9,lambda=2.4)## [1] 4\nrpois(n=10,lambda=2.4)##  [1] 2 4 4 6 0 3 3 2 2 1\n par(mfrow = c(2, 1))\n x <- seq(-0.01, 5, 0.01)\n plot(x, ppois(x, 1), type=\"s\", ylab=\"F(x)\", main=\"Poisson(1) CDF\")\n plot(x, pbinom(x, 100, 0.01),type=\"s\", ylab=\"F(x)\",main=\"Binomial(100, 0.01) CDF\")\ndnorm(0,mean=0,sd=1)## [1] 0.3989423\npnorm(0)## [1] 0.5\nqnorm(2.5/100,lower.tail=F)## [1] 1.959964\nrnorm(10,mean=1,sd=1.5)##  [1]  0.9136374  0.5886706 -1.1184606 -0.7624067  2.5011198  2.6856772\n##  [7]  3.0430825  0.6944923  1.5424163  1.6967752\nx <- seq(-4, 4, length = 401)\nplot(x, dnorm(x), type = 'l') # N(0, 1)\n# N(1, 1.5^2):\nlines(x, dnorm(x, mean = 1, sd = 1.5), lty = 'dashed')\nu <- seq(0, 1, length=401)\nplot(u, qnorm(u), 'l')\n# lower.tail = FALSE gives q(1-u)\nlines(u, qnorm(u, lower.tail = FALSE), lty = 'dashed')\nn<-1000\nx<-rnorm(n)\nxh<-hist(x, probability = TRUE)\nxh## $breaks\n##  [1] -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5\n## \n## $counts\n##  [1]   2   2  21  45  86 137 230 174 153  88  44  12   5   1\n## \n## $density\n##  [1] 0.004 0.004 0.042 0.090 0.172 0.274 0.460 0.348 0.306 0.176 0.088 0.024\n## [13] 0.010 0.002\n## \n## $mids\n##  [1] -3.25 -2.75 -2.25 -1.75 -1.25 -0.75 -0.25  0.25  0.75  1.25  1.75  2.25\n## [13]  2.75  3.25\n## \n## $xname\n## [1] \"x\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\nz<-seq(-3,3,0.01)\ny <- dnorm(z, mean = 0, sd = 1)\nlines(x = z, y = y, col = \"blue\")\nq = c(.2,.5,.1,.1,.1)\np = 0.5\ndexp(x, rate = 1, log = FALSE)##  [1] 0.0000000 0.6467677 0.2709752 0.6514087 0.4451820 0.0000000 0.0000000\n##  [8] 0.3937035 0.0000000 0.0000000 0.0000000 0.8142085 0.2947052 0.7195265\n## [15] 0.4658846 0.5511078 0.0000000 0.9246197 0.6969978 0.0000000 0.0000000\n## [22] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n## [29] 0.1963103 0.0000000\n##  [ reached getOption(\"max.print\") -- omitted 970 entries ]\npexp(q, rate = 1, lower.tail = TRUE, log.p = FALSE)## [1] 0.18126925 0.39346934 0.09516258 0.09516258 0.09516258\nqexp(p, rate = 1, lower.tail = TRUE, log.p = FALSE)## [1] 0.6931472\nrexp(n, rate = 1)##  [1] 0.005536359 0.207342347 0.599133480 1.170997404 0.290882872 0.177673159\n##  [7] 1.246424135 0.175841576 0.041281463 0.885665916 0.018615517 0.615877023\n## [13] 1.534591656 3.264871947 1.153509116 0.796585224 1.705156151 0.473271821\n## [19] 1.314082502 1.282413484 2.387044230 2.386808468 0.748855574 0.488763332\n## [25] 1.389614921 1.573671862 0.485501722 1.378829610 0.805851386 0.522162373\n##  [ reached getOption(\"max.print\") -- omitted 970 entries ]\ndunif(x, min=0, max=1, log = FALSE)##  [1] 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n##  [ reached getOption(\"max.print\") -- omitted 970 entries ]\npunif(q, min=0, max=1, lower.tail = TRUE, log.p = FALSE)## [1] 0.2 0.5 0.1 0.1 0.1\nqunif(p, min=0, max=1, lower.tail = TRUE, log.p = FALSE)## [1] 0.5\nrunif(n, min=0, max=1)##  [1] 0.77693961 0.15614217 0.33720060 0.79972727 0.10672545 0.62887067\n##  [7] 0.17407200 0.23192336 0.08219918 0.35697951 0.07556067 0.39823420\n## [13] 0.27835237 0.63720385 0.84284525 0.18321810 0.14768553 0.21428431\n## [19] 0.87226437 0.89715272 0.63679677 0.49365256 0.75080120 0.06441163\n## [25] 0.71274235 0.78788034 0.58013062 0.52959147 0.58936567 0.84903366\n##  [ reached getOption(\"max.print\") -- omitted 970 entries ]"},{"path":"probability-statistics.html","id":"exponential-distribution","chapter":"第 3 章 概率论和数理统计","heading":"3.5 exponential distribution","text":"http://personal.kenyon.edu/hartlaub/MellonProject/Bivariate2.html","code":"\n#cumulative distribution function\ncurve(pexp(x,rate=0.5), xlim=c(0,10), col=1, lwd=3,\n      main='Exponential Probability Distribution Function')\ncurve(pexp(x,rate=1), xlim=c(0,10), col=2, lwd=2, lty=2,\n      add=T)\ncurve(pexp(x,rate=5), xlim=c(0,10), col=3, lwd=2, lty=3,\n      add=T)\ncurve(pexp(x,rate=10), xlim=c(0,10), col=4, lwd=2, lty=4,\n      add=T)\nlegend(par('usr')[2], par('usr')[4], xjust=1,\n       c('rate=0.5','rate=1', 'rate=2','rate=10'),\n       lwd=2, lty=c(1,2,3,4),\n       col=1:4)\n#density\ncurve(dexp(x,rate=0.5), xlim=c(0,10), col=1, lwd=3,\n      main='Exponential Probability Distribution Function')\ncurve(dexp(x,rate=1), xlim=c(0,10), col=2, lwd=2, lty=2,\n      add=T)\ncurve(dexp(x,rate=5), xlim=c(0,10), col=3, lwd=2, lty=3,\n      add=T)\ncurve(dexp(x,rate=10), xlim=c(0,10), col=4, lwd=2, lty=4,\n      add=T)\nlegend(par('usr')[2], par('usr')[4], xjust=1,\n       c('rate=0.5','rate=1', 'rate=2','rate=10'),\n       lwd=2, lty=1:4,\n       col=1:4)\n###normal\n#cumulative distribution function\ncurve(pnorm(x), xlim=c(-5,5), col='red', lwd=3)\ntitle(main='Cumulative gaussian distribution function')\ncurve(pnorm(x,1,1), xlim=c(-5,5), col='green', lwd=3,add=T)\ncurve(pnorm(x,1,2),  xlim=c(-5,5), col='black', lwd=3,add=T)\nlegend(-par('usr')[2], par('usr')[4], xjust=-0.5,\n       c('standard norm', 'normal(1,1)','normal(1,2)'),\n       lwd=2, col=c('red','green','black'))\n#density\ncurve(dnorm(x), xlim=c(-5,5), col='red', lwd=3)\ncurve(dnorm(x,1,1), add=T, col='green', lty=2, lwd=3)\ncurve(dnorm(x,1,2), add=T, col='black', lty=3, lwd=3)\n\nlegend(par('usr')[2], par('usr')[4], xjust=1,\n       c('standard normal', 'normal(1,1)','normal(1,2)'),\n       lwd=2, lty=c(1,2,3),\n       col=c('red','green','black'))\n###mixture of normal\n \nm <- c(-2,0,2)    # Means\np <- c(.3,.4,.3)  # Probabilities\ns <- c(1, 1, 1)   # Standard deviations\n \ncurve( p[2]*dnorm(x, mean=m[2], sd=s[2]),\n       col = \"green\", lwd = 3, \n       xlim = c(-5,5),ylim=c(0,0.23),\n       main = \"The three gaussian distributions in our mixture\",\n       xlab = \"\", ylab = \"\")\ncurve( p[1]*dnorm(x, mean=m[1], sd=s[1]),\n       col=\"red\", lwd=3, add=TRUE)\ncurve( p[3]*dnorm(x, mean=m[3], sd=s[3]),\n       col=\"blue\", lwd=3, add=TRUE)\n\n\ncurve(p[1]*dnorm(x, mean=m[1], sd=s[1])+p[2]*dnorm(x, mean=m[2], sd=s[2])+p[3]*dnorm(x, mean=m[3], sd=s[3]),col=\"black\", lwd=3, add=TRUE)\n### bivariate normal density with matlab\n\n\n###Plot of mixtures of bivariate normal with R\n\n# install.packages(\"rgl\")\nlibrary(rgl)\n \ndnorm2d<-function(x,y,mu1,mu2,sigma1,sigma2,rho){\n    xoy = ((x-mu1)^2/sigma1^2 - 2*rho * (x-mu1)/sigma1 * (y-mu2)/sigma2 + (y-mu2)^2/sigma2^2)/(2 * (1 - rho^2))\n    density = exp(-xoy)/(2 * pi *sigma1*sigma2*sqrt(1 - rho^2))\n    density\n}\n\n\n\nx<-seq(-5,5,by=0.1)\ny<-seq(-5,5,by=0.1)\n\nff1<-function(x,y){0.5*dnorm2d(x,y,0,0,1,1,0)+0.5*dnorm2d(x,y,0,0,1,1,0.5)}\n\nff2<-function(x,y){0.5*dnorm2d(x,y,0,0,1,1,0.5)+0.5*dnorm2d(x,y,0,0,1,1,-0.5)}\n\n\nff3<-function(x,y){0.3*dnorm2d(x,y,0,0,1,1,0)+0.7*dnorm2d(x,y,2.5,2.5,1.75,1.75,0)}\n\nopen3d() # This will open a small window where you can plot 3D figures on.\n\nz<-outer(x,y,ff1) \npersp3d(x,y,z,col=\"green\",main=\"ff1\")\n\nopen3d()\nz<-outer(x,y,ff2)\npersp3d(x,y,z,col=\"green\",main=\"ff2\")\n\nopen3d()\nz<-outer(x,y,ff3)\npersp3d(x,y,z,col=\"green\",main=\"ff3\")"},{"path":"data-mining-practise.html","id":"data-mining-practise","chapter":"第 4 章 关于《数据挖掘实战》","heading":"第 4 章 关于《数据挖掘实战》","text":"该部分内容是学习《数据挖掘——基于 R 语言的实战》一书时做的读书笔记。作者简介张俊妮，美国哈佛大学统计学博士，现任北京大学国家发展研究院副教授。主要研究领域：人口统计学、数据挖掘与文本挖掘、因果推断。有十余年给北京大学各学科的学生讲授数据挖掘课程的经验，曾获北京大学教学优秀奖。关于本书的内容本书以深入浅出的语言系统地介绍了数据挖掘的框架和基本方法，主要内容包括：数据挖掘与 R 语言概述、数据理解、数据准备、关联规则挖掘、聚类分析、线性模型与广义线性模型、神经网络的基本方法、决策树、基于决策树的模型组合、模型评估与比较、R 语言数据挖掘大案例。本书使用基于 R 语言的数据挖掘案例贯穿全书，并辅以上机实验和习题，帮助读者熟练使用 R 语言进行数据挖掘。","code":""},{"path":"association-analysis.html","id":"association-analysis","chapter":"第 5 章 关联规则挖掘","heading":"第 5 章 关联规则挖掘","text":"关联规则挖掘常用于购物篮分析应用，目的是发现有意义的强关联规则。","code":""},{"path":"association-analysis.html","id":"购物篮分析","chapter":"第 5 章 关联规则挖掘","heading":"5.1 购物篮分析","text":"使用 arules 软件包自带的 Groceries 交易（transaction）数据集进行购物篮分析。arules 是用于关联规则挖掘的程序包，我们将调用其中的 itemFrequencyPlot()、apriori()、inspect() 等函数。arulesViz 是用于关联规则可视化的程序包，我们将调用其中的 plot() 函数。","code":"\ninstall.packages(\"arules\")\ninstall.packages(\"arulesViz\")\nlibrary(arules)\nlibrary(arulesViz)"},{"path":"association-analysis.html","id":"交易数据集的类","chapter":"第 5 章 关联规则挖掘","heading":"5.1.1 交易数据集的类","text":"Groceries 交易数据集是一个结构化数据，包含了对 169 种商品的 9835 次购买记录。使用 head() 和 inspect() 函数可以查看 transactions 对象中的交易数据。交易数据中，购买次数最多的商品是 whole milk，有超过 2500 次；其次是 vegetables 等等（图 5.1）。\n图 5.1: 交易数据集中购买次数最多的 20 项\n","code":"\ndata(\"Groceries\")\nGroceries## transactions in sparse format with\n##  9835 transactions (rows) and\n##  169 items (columns)\nstr(Groceries)## Formal class 'transactions' [package \"arules\"] with 3 slots\n##   ..@ data       :Formal class 'ngCMatrix' [package \"Matrix\"] with 5 slots\n##   .. .. ..@ i       : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ...\n##   .. .. ..@ p       : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ...\n##   .. .. ..@ Dim     : int [1:2] 169 9835\n##   .. .. ..@ Dimnames:List of 2\n##   .. .. .. ..$ : NULL\n##   .. .. .. ..$ : NULL\n##   .. .. ..@ factors : list()\n##   ..@ itemInfo   :'data.frame':  169 obs. of  3 variables:\n##   .. ..$ labels: chr [1:169] \"frankfurter\" \"sausage\" \"liver loaf\" \"ham\" ...\n##   .. ..$ level2: Factor w/ 55 levels \"baby food\",\"bags\",..: 44 44 44 44 44 44 44 42 42 41 ...\n##   .. ..$ level1: Factor w/ 10 levels \"canned food\",..: 6 6 6 6 6 6 6 6 6 6 ...\n##   ..@ itemsetInfo:'data.frame':  0 obs. of  0 variables\nsummary(Groceries)## transactions as itemMatrix in sparse format with\n##  9835 rows (elements/itemsets/transactions) and\n##  169 columns (items) and a density of 0.02609146 \n## \n## most frequent items:\n##       whole milk other vegetables       rolls/buns             soda \n##             2513             1903             1809             1715 \n##           yogurt          (Other) \n##             1372            34055 \n## \n## element (itemset/transaction) length distribution:\n## sizes\n##    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n## 2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n##   17   18   19   20   21   22   23   24   26   27   28   29   32 \n##   29   14   14    9   11    4    6    1    1    1    1    3    1 \n## \n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.000   2.000   3.000   4.409   6.000  32.000 \n## \n## includes extended item information - examples:\n##        labels  level2           level1\n## 1 frankfurter sausage meat and sausage\n## 2     sausage sausage meat and sausage\n## 3  liver loaf sausage meat and sausage\ninspect(head(Groceries))##     items                     \n## [1] {citrus fruit,            \n##      semi-finished bread,     \n##      margarine,               \n##      ready soups}             \n## [2] {tropical fruit,          \n##      yogurt,                  \n##      coffee}                  \n## [3] {whole milk}              \n## [4] {pip fruit,               \n##      yogurt,                  \n##      cream cheese ,           \n##      meat spreads}            \n## [5] {other vegetables,        \n##      whole milk,              \n##      condensed milk,          \n##      long life bakery product}\n## [6] {whole milk,              \n##      butter,                  \n##      yogurt,                  \n##      rice,                    \n##      abrasive cleaner}\nitemFrequencyPlot(Groceries, topN = 20, type = \"absolute\")"},{"path":"association-analysis.html","id":"对交易数据集进行关联分析","chapter":"第 5 章 关联规则挖掘","heading":"5.1.2 对交易数据集进行关联分析","text":"apriori() 函数使用 Apriori 算法挖掘关联规则1。函数参数中的 parameter 参数可以指定关联规则的支持度（support），置信度（confidence），每条规则包含的最大项数和最小项数（maxlen/minlen）以及输出结果格式（target）等。查看输出规则的基本信息，可知：所生成的关联规则共有 410 条；所生成的关联规则共有 410 条；关联规则的长度（前项集 lhs 的项数2 + 后项集 rhs 的项数）大多数为 4 或 5；关联规则的长度（前项集 lhs 的项数2 + 后项集 rhs 的项数）大多数为 4 或 5；关联规则的支持度、置信度、提升值和支持观测数的统计值（分位数等）关联规则的支持度、置信度、提升值和支持观测数的统计值（分位数等）关联规则挖掘的信息。关联规则挖掘的信息。按照提升值（lift）取关联规则的前 3 项，结果显示由项集 \\(\\)（{liquor, red/blush wine}） 指向项集 \\(B\\)（{bottled beer} ）提升值最高。图 5.2 对关联规则的可视化结果中可以看出，在全部的 410 个强关联规则中，存在一些支持度（横轴）、置信度（纵轴）和提升值（颜色）均比较理想的规则。\n图 5.2: 关联规则的可视化\n","code":"\nrules = apriori(Groceries,\n                parameter = list(\n                  support = 0.001,\n                  confidence = 0.8,\n                  target = \"rules\"\n                ))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support minlen\n##         0.8    0.1    1 none FALSE            TRUE       5   0.001      1\n##  maxlen target  ext\n##      10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 9 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\n## sorting and recoding items ... [157 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 done [0.01s].\n## writing ... [410 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nsummary(rules)## set of 410 rules\n## \n## rule length distribution (lhs + rhs):sizes\n##   3   4   5   6 \n##  29 229 140  12 \n## \n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   3.000   4.000   4.000   4.329   5.000   6.000 \n## \n## summary of quality measures:\n##     support           confidence        coverage             lift       \n##  Min.   :0.001017   Min.   :0.8000   Min.   :0.001017   Min.   : 3.131  \n##  1st Qu.:0.001017   1st Qu.:0.8333   1st Qu.:0.001220   1st Qu.: 3.312  \n##  Median :0.001220   Median :0.8462   Median :0.001322   Median : 3.588  \n##  Mean   :0.001247   Mean   :0.8663   Mean   :0.001449   Mean   : 3.951  \n##  3rd Qu.:0.001322   3rd Qu.:0.9091   3rd Qu.:0.001627   3rd Qu.: 4.341  \n##  Max.   :0.003152   Max.   :1.0000   Max.   :0.003559   Max.   :11.235  \n##      count      \n##  Min.   :10.00  \n##  1st Qu.:10.00  \n##  Median :12.00  \n##  Mean   :12.27  \n##  3rd Qu.:13.00  \n##  Max.   :31.00  \n## \n## mining info:\n##       data ntransactions support confidence\n##  Groceries          9835   0.001        0.8\n##                                                                                              call\n##  apriori(data = Groceries, parameter = list(support = 0.001, confidence = 0.8, target = \"rules\"))\ninspect(head(rules, by = \"lift\", n = 3))##     lhs                        rhs                   support confidence    coverage     lift count\n## [1] {liquor,                                                                                      \n##      red/blush wine}        => {bottled beer}    0.001931876  0.9047619 0.002135231 11.23527    19\n## [2] {citrus fruit,                                                                                \n##      other vegetables,                                                                            \n##      soda,                                                                                        \n##      fruit/vegetable juice} => {root vegetables} 0.001016777  0.9090909 0.001118454  8.34040    10\n## [3] {tropical fruit,                                                                              \n##      other vegetables,                                                                            \n##      whole milk,                                                                                  \n##      yogurt,                                                                                      \n##      oil}                   => {root vegetables} 0.001016777  0.9090909 0.001118454  8.34040    10\nplot(rules)"},{"path":"association-analysis.html","id":"对关联规则去冗余","chapter":"第 5 章 关联规则挖掘","heading":"5.1.3 对关联规则去冗余","text":"关联规则中可能有一些是冗余的。举例来说：假设有一个规则 {, B, C, D, E} => {F}，其置信度为 0.9；另有一条规则 {, B, C, D} => {F}，其置信度为 0.95。因为后面这个规则的一般化程度和置信度都更高，所以前一条就是冗余的规则。除了置信度，还可以用提升值来判断一条规则是否冗余。","code":"\n# 去冗余\nrules_pruned = rules[!is.redundant(rules)]"},{"path":"association-analysis.html","id":"对关联规则进行排序和控制","chapter":"第 5 章 关联规则挖掘","heading":"5.1.4 对关联规则进行排序和控制","text":"根据置信度对输出规则进行排序。控制关联规则的长度。","code":"\nrules = sort(rules, by = \"confidence\", decreasing = TRUE)\n# 指定规则长度最大为 3\nrules_maxlen = apriori(\n  Groceries,\n  parameter = list(\n    supp = 0.001,\n    conf = 0.8,\n    maxlen = 3\n  )\n)## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support minlen\n##         0.8    0.1    1 none FALSE            TRUE       5   0.001      1\n##  maxlen target  ext\n##       3  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 9 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\n## sorting and recoding items ... [157 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 done [0.00s].\n## writing ... [29 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\n# 查看规则\ninspect(head(rules_maxlen, by = \"lift\", n = 2))##     lhs                         rhs                support     confidence\n## [1] {liquor, red/blush wine} => {bottled beer}     0.001931876 0.9047619 \n## [2] {grapes, onions}         => {other vegetables} 0.001118454 0.9166667 \n##     coverage    lift      count\n## [1] 0.002135231 11.235269 19   \n## [2] 0.001220132  4.737476 11"},{"path":"association-analysis.html","id":"指定前项集和后项集","chapter":"第 5 章 关联规则挖掘","heading":"5.1.5 指定前项集和后项集","text":"指定前项集可以探索顾客在购买了某一商品后，可能会继续购买什么商品。如下面的例子，可以发现用户在购买了 whole milk 后，更有可能去买点蔬菜、酸奶等商品（图 5.3）。\n图 5.3: 指定前项集的关联规则\n指定后项集可以探索顾客在什么情况下会去买一个商品。如下面的例子，可以发现用户在购买了 点 whipped/sour cream、root vegetables 等商品后，更有可能去买 whole milk（图 5.4）。\n图 5.4: 指定后项集的关联规则\n","code":"\nrules_lhs_wholemilk = apriori(\n  Groceries,\n  parameter = list(\n    supp = 0.001,\n    conf = 0.15\n  ),\n  appearance = list(\n    lhs = \"whole milk\"\n  ),\n  control = list(\n    verbose = FALSE\n  )\n)\n\nplot(head(rules_lhs_wholemilk,by = \"lift\"), method = \"graph\")\nrules_rhs_wholemilk = apriori(\n  Groceries,\n  parameter = list(\n    supp = 0.001,\n    conf = 0.8\n  ),\n  appearance = list(\n    rhs = \"whole milk\"\n  ),\n  control = list(\n    verbose = FALSE\n  )\n)\n\nplot(head(rules_rhs_wholemilk,by = \"lift\"), method = \"graph\")"},{"path":"association-analysis.html","id":"tatanic-survival","chapter":"第 5 章 关联规则挖掘","heading":"5.2 泰坦尼克号存活情况分析","text":"","code":""},{"path":"association-analysis.html","id":"数据集的定义和预处理","chapter":"第 5 章 关联规则挖掘","heading":"5.2.1 数据集的定义和预处理","text":"本分析所用的数据集记录了 891 位泰坦尼克号乘客的 12 个变量信息，希望能够通过分析挖掘乘客的存活情况与其它变量的关联规则。该数据集包含的字段如下：PassengerId：乘客编号Survived：是否存活（0 = 没有，1 = 存活）Pclass：船舱等级（1 = 一等舱，2 = 二等舱，3 = 三等舱）Name：乘客姓名Sex：乘客性别Age：乘客年龄Sibsp：船上的兄妹/配偶数目Parch：船上的父母/孩子数目Ticket：船票号Fare：船票价格Cabin：船舱号Embarked：登船港口（C = 瑟堡，Q = 皇后镇，S = 南安普顿）在进行关联规则分析前，对数据集进行如下预处理：去掉唯一标识符字段，如乘客编号（PassengerId）、姓名（Name）、传票号（Ticket）；去掉缺失过多的字段，如船舱号（Cabin）；去掉冗余的字段，如因为船票价格（Fare）与船舱等级相关（Pclass），故删去；删掉一些不完整的记录，如年龄（Age）缺失的观测。经过这些处理后，还有 712 条观测数据，每个观测有 7 个值。因为关联分析需要所有的变量均为分类变量（因子），所以需要把年龄（Age）切分为因子。在进行这一操作之前，先看一下年龄的分布情况。接下来将年龄以 20 岁为间隔切分，并将所有变量转变为因子。","code":"\nlibrary(readr)\nfile = xfun::magic_path(\"ch4_titanic_train.csv\")\ntitanic = read_csv(file)\nsummary(titanic)##   PassengerId       Survived          Pclass          Name          \n##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        \n##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  \n##      Sex                 Age            SibSp           Parch       \n##  Length:891         Min.   : 0.42   Min.   :0.000   Min.   :0.0000  \n##  Class :character   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  \n##     Ticket               Fare           Cabin             Embarked        \n##  Length:891         Min.   :  0.00   Length:891         Length:891        \n##  Class :character   1st Qu.:  7.91   Class :character   Class :character  \n##  [ reached getOption(\"max.print\") -- omitted 5 rows ]\nlibrary(dplyr)\ntitanic = titanic %>%\n  select(-c(PassengerId, Name, Ticket, Cabin, Fare)) %>%\n  mutate_if(is.character, list(~na_if(., \"\"))) %>% # 将空字符串替换为 NA\n  filter(complete.cases(.))\ntitanic## # A tibble: 712 × 7\n##    Survived Pclass Sex      Age SibSp Parch Embarked\n##       <dbl>  <dbl> <chr>  <dbl> <dbl> <dbl> <chr>   \n##  1        0      3 male      22     1     0 S       \n##  2        1      1 female    38     1     0 C       \n##  3        1      3 female    26     0     0 S       \n##  4        1      1 female    35     1     0 S       \n##  5        0      3 male      35     0     0 S       \n##  6        0      1 male      54     0     0 S       \n##  7        0      3 male       2     3     1 S       \n##  8        1      3 female    27     0     2 S       \n##  9        1      2 female    14     1     0 C       \n## 10        1      3 female     4     1     1 S       \n## # … with 702 more rows\n## # ℹ Use `print(n = ...)` to see more rows\nlibrary(ggplot2)\nggplot(titanic, aes(Age)) +\n  geom_histogram(binwidth = 5, fill = \"lightblue\", color = \"black\")\ntitanic = titanic %>%\n  mutate(Age = cut(Age, breaks = seq(0,100,by=20))) %>%\n  mutate_all(as.factor)"},{"path":"association-analysis.html","id":"关联规则分析","chapter":"第 5 章 关联规则挖掘","heading":"5.2.2 关联规则分析","text":"接下来使用 apriori() 函数对 titanic 数据集进行关联分析。","code":"\nlibrary(arules)\nlibrary(arulesViz)\nrules = apriori(titanic, parameter = list(\n  support = 0.1,\n  confidence = 0.8\n))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support minlen\n##         0.8    0.1    1 none FALSE            TRUE       5     0.1      1\n##  maxlen target  ext\n##      10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 71 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[27 item(s), 712 transaction(s)] done [0.00s].\n## sorting and recoding items ... [16 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 done [0.00s].\n## writing ... [253 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nsummary(rules)## set of 253 rules\n## \n## rule length distribution (lhs + rhs):sizes\n##  2  3  4  5  6  7 \n##  8 55 89 67 29  5 \n## \n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   2.000   4.000   4.000   4.273   5.000   7.000 \n## \n## summary of quality measures:\n##     support         confidence        coverage           lift      \n##  Min.   :0.1011   Min.   :0.8030   Min.   :0.1110   Min.   :1.034  \n##  1st Qu.:0.1559   1st Qu.:0.8377   1st Qu.:0.1756   1st Qu.:1.115  \n##  Median :0.2191   Median :0.8549   Median :0.2500   Median :1.275  \n##  Mean   :0.2326   Mean   :0.8706   Mean   :0.2692   Mean   :1.262  \n##  3rd Qu.:0.2851   3rd Qu.:0.9027   3rd Qu.:0.3258   3rd Qu.:1.369  \n##  Max.   :0.5646   Max.   :1.0000   Max.   :0.6587   Max.   :2.383  \n##      count      \n##  Min.   : 72.0  \n##  1st Qu.:111.0  \n##  Median :156.0  \n##  Mean   :165.6  \n##  3rd Qu.:203.0  \n##  Max.   :402.0  \n## \n## mining info:\n##     data ntransactions support confidence\n##  titanic           712     0.1        0.8\n##                                                                        call\n##  apriori(data = titanic, parameter = list(support = 0.1, confidence = 0.8))\nplot(rules)\nrules = rules[is.redundant(rules)]\ninspect(head(rules, by = \"lift\"))##     lhs               rhs            support confidence  coverage     lift count\n## [1] {Pclass=3,                                                                  \n##      Sex=male,                                                                  \n##      SibSp=0,                                                                   \n##      Parch=0,                                                                   \n##      Embarked=S}   => {Survived=0} 0.1952247  0.8687500 0.2247191 1.458844   139\n## [2] {Survived=0,                                                                \n##      Pclass=2,                                                                  \n##      Embarked=S}   => {Sex=male}   0.1067416  0.9268293 0.1151685 1.456738    76\n## [3] {Survived=0,                                                                \n##      Age=(20,40],                                                               \n##      SibSp=0,                                                                   \n##      Parch=0,                                                                   \n##      Embarked=S}   => {Sex=male}   0.1797753  0.9208633 0.1952247 1.447361   128\n## [4] {Survived=0,                                                                \n##      Age=(20,40],                                                               \n##      SibSp=0,                                                                   \n##      Parch=0}      => {Sex=male}   0.2106742  0.9202454 0.2289326 1.446390   150\n## [5] {Pclass=3,                                                                  \n##      Sex=male,                                                                  \n##      SibSp=0,                                                                   \n##      Parch=0}      => {Survived=0} 0.2261236  0.8609626 0.2626404 1.445767   161\n## [6] {Pclass=3,                                                                  \n##      Sex=male,                                                                  \n##      Age=(20,40],                                                               \n##      Parch=0,                                                                   \n##      Embarked=S}   => {Survived=0} 0.1418539  0.8559322 0.1657303 1.437320   101"},{"path":"association-analysis.html","id":"获救的关联特征","chapter":"第 5 章 关联规则挖掘","heading":"5.2.3 获救的关联特征","text":"使用指定后项集的关联规则挖掘，可以探究顾客特征取什么值时会存活（Survived = 1）。分析显示，头等舱、女性、无父母和子女的乘客获救的提升值最高，总体有 98.1% 的几率存活；头等舱、女性、年龄 20 - 40 岁的乘客则有 97.7% 的几率存活。","code":"\nrules_rhs_survive = apriori(\n  titanic,\n  parameter = list(supp = 0.05, conf = 0.8),\n  appearance = list(rhs = c(\"Survived=1\")),\n  control = list(verbose = FALSE)\n)\nrules_rhs_survive = rules_rhs_survive[!is.redundant(rules_rhs_survive)]\ninspect(head(rules_rhs_survive, by = \"lift\"))##     lhs                                    rhs          support    confidence\n## [1] {Pclass=1, Sex=female, Parch=0}     => {Survived=1} 0.07443820 0.9814815 \n## [2] {Pclass=1, Sex=female, Age=(20,40]} => {Survived=1} 0.06039326 0.9772727 \n## [3] {Pclass=1, Sex=female, SibSp=0}     => {Survived=1} 0.06039326 0.9772727 \n##     coverage   lift     count\n## [1] 0.07584270 2.426440 53   \n## [2] 0.06179775 2.416035 43   \n## [3] 0.06179775 2.416035 43   \n##  [ reached 'max' / getOption(\"max.print\") -- omitted 3 rows ]"},{"path":"association-analysis.html","id":"死亡的关联特征","chapter":"第 5 章 关联规则挖掘","heading":"5.2.4 死亡的关联特征","text":"分析乘客死亡的关联特征，可以得出：具有二等舱、男性、年龄在 20 - 40 岁、无兄弟姐妹的乘客死亡几率可达 95%。","code":"\nrules_rhs_not_survive = apriori(\n  titanic,\n  parameter = list(supp = 0.05, conf = 0.8),\n  appearance = list(rhs = c(\"Survived=0\")),\n  control = list(verbose = FALSE)\n)\nrules_rhs_not_survive = rules_rhs_not_survive[!is.redundant(rules_rhs_not_survive)]\ninspect(head(rules_rhs_not_survive, by = \"lift\"))##     lhs               rhs             support confidence   coverage     lift count\n## [1] {Pclass=2,                                                                    \n##      Sex=male,                                                                    \n##      Age=(20,40],                                                                 \n##      SibSp=0}      => {Survived=0} 0.05337079  0.9500000 0.05617978 1.595283    38\n## [2] {Pclass=2,                                                                    \n##      Sex=male,                                                                    \n##      Age=(20,40]}  => {Survived=0} 0.07865169  0.9491525 0.08286517 1.593860    56\n## [3] {Pclass=2,                                                                    \n##      Sex=male,                                                                    \n##      Parch=0}      => {Survived=0} 0.10393258  0.9250000 0.11235955 1.553302    74\n## [4] {Sex=male,                                                                    \n##      Age=(0,20],                                                                  \n##      SibSp=0,                                                                     \n##      Parch=0,                                                                     \n##      Embarked=S}   => {Survived=0} 0.05477528  0.9069767 0.06039326 1.523036    39\n## [5] {Pclass=2,                                                                    \n##      Sex=male,                                                                    \n##      SibSp=0,                                                                     \n##      Embarked=S}   => {Survived=0} 0.08005618  0.9047619 0.08848315 1.519317    57\n## [6] {Sex=male,                                                                    \n##      Age=(0,20],                                                                  \n##      Parch=0,                                                                     \n##      Embarked=S}   => {Survived=0} 0.06039326  0.8958333 0.06741573 1.504324    43"},{"path":"association-analysis.html","id":"学生特征及考试成绩","chapter":"第 5 章 关联规则挖掘","heading":"5.3 学生特征及考试成绩","text":"StudentsPerformance.csv 数据集给出了一些学生的特征及考试成绩。数据的定义如下：gender：性别race/ethnicity:种族parental level education：父母教育水平lunch：参与的午餐计划test preparation course：测验准备课程完成情况math score：数学成绩reading score：阅读成绩writing score：写作成绩将成绩分为不及格（< 60），及格（＜ 85）和优秀（≥ 85）3 个等级，然后做关联规则分析。例如数学成绩及格的一些关联特征如下。","code":"\nfile = xfun::magic_path(\"StudentsPerformance.csv\")\nstudentPerformance = readr::read_csv(file)\nstudentPerformance## # A tibble: 1,000 × 8\n##    gender `race/ethnicity` parental leve…¹ lunch test …² math …³ readi…⁴ writi…⁵\n##    <chr>  <chr>            <chr>           <chr> <chr>     <dbl>   <dbl>   <dbl>\n##  1 female group B          bachelor's deg… stan… none         72      72      74\n##  2 female group C          some college    stan… comple…      69      90      88\n##  3 female group B          master's degree stan… none         90      95      93\n##  4 male   group A          associate's de… free… none         47      57      44\n##  5 male   group C          some college    stan… none         76      78      75\n##  6 female group B          associate's de… stan… none         71      83      78\n##  7 female group B          some college    stan… comple…      88      95      92\n##  8 male   group B          some college    free… none         40      43      39\n##  9 male   group D          high school     free… comple…      64      64      67\n## 10 female group B          high school     free… none         38      60      50\n## # … with 990 more rows, and abbreviated variable names\n## #   ¹​`parental level of education`, ²​`test preparation course`, ³​`math score`,\n## #   ⁴​`reading score`, ⁵​`writing score`\n## # ℹ Use `print(n = ...)` to see more rows\nlibrary(dplyr)\nstudentPerformance = studentPerformance %>%\n  mutate_if(is.numeric, ~ cut(., breaks = c(0, 59, 84, 100), include.lowest = TRUE)) %>%\n  mutate_all(as.factor)\n\n# 关联分析\nrules_rhs_math = apriori(studentPerformance,\n                parameter = list(\n                  supp = 0.1,\n                  conf = 0.5\n                ),\n                appearance = list(rhs = \"math score=(59,84]\")\n                )## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support minlen\n##         0.5    0.1    1 none FALSE            TRUE       5     0.1      1\n##  maxlen target  ext\n##      10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 100 \n## \n## set item appearances ...[1 item(s)] done [0.00s].\n## set transactions ...[26 item(s), 1000 transaction(s)] done [0.00s].\n## sorting and recoding items ... [24 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 done [0.00s].\n## writing ... [78 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\ninspect(head(rules_rhs_math, by = \"lift\"))##     lhs                                rhs                  support confidence coverage     lift count\n## [1] {gender=male,                                                                                     \n##      reading score=(59,84],                                                                           \n##      writing score=(59,84]}         => {math score=(59,84]}   0.207  0.8553719    0.242 1.527450   207\n## [2] {gender=male,                                                                                     \n##      test preparation course=none,                                                                    \n##      reading score=(59,84],                                                                           \n##      writing score=(59,84]}         => {math score=(59,84]}   0.120  0.8510638    0.141 1.519757   120\n## [3] {gender=male,                                                                                     \n##      lunch=standard,                                                                                  \n##      reading score=(59,84],                                                                           \n##      writing score=(59,84]}         => {math score=(59,84]}   0.143  0.8461538    0.169 1.510989   143\n## [4] {gender=male,                                                                                     \n##      test preparation course=none,                                                                    \n##      reading score=(59,84]}         => {math score=(59,84]}   0.141  0.8294118    0.170 1.481092   141\n## [5] {gender=male,                                                                                     \n##      reading score=(59,84]}         => {math score=(59,84]}   0.233  0.8233216    0.283 1.470217   233\n## [6] {gender=male,                                                                                     \n##      lunch=standard,                                                                                  \n##      test preparation course=none,                                                                    \n##      reading score=(59,84]}         => {math score=(59,84]}   0.100  0.8196721    0.122 1.463700   100"},{"path":"association-analysis.html","id":"关联规则分析的重要概念","chapter":"第 5 章 关联规则挖掘","heading":"5.4 关联规则分析的重要概念","text":"关联分析中的最小支持度、最小置信度等参数的设置对于关联分析的结果由显著影响。它们的定义如下：支持度：相当于联合概率。例如，若有 3% 的顾客同时购买了尿布与啤酒，那么 {尿布} => {啤酒} 的支持度为 3%。置信度：相当于条件概率。例如，若购买了尿布的顾客有 20% 购买了啤酒，那么 {尿布} => {啤酒} 的置信度为 20%。术语中英文对照项集（itemset）：项的集合。频繁项集（frequent itemset）","code":""},{"path":"clustering-analysis.html","id":"clustering-analysis","chapter":"第 6 章 聚类分析","heading":"第 6 章 聚类分析","text":"聚类分析是一种无监督数据挖掘方法，它基于观测之间的距离度量将观测分组。聚类分析可用于对客户进行细分，以便为细分客户群体指定针对性营销策略。常用的聚类方法有： \\(k\\) 均值聚类法和层次聚类法。","code":""},{"path":"clustering-analysis.html","id":"商场客户聚类","chapter":"第 6 章 聚类分析","heading":"6.1 商场客户聚类","text":"ch5_mall.csv 数据集记录了一家商场的 200 位客户的信息。这里使用客户年龄（Age）、年收入（Annual Income）和消费得分（Spending Score）对客户进行聚类。为了度量两个观测之间的距离，通常在聚类前对各连续变量进行标准化。scale() 函数的参数 center = TURE 表示减去均值，scale = TRUE 表示除以标准偏差。因此标准化后的数据均值为 0，标准差为 1。","code":"\nfile = xfun::magic_path(\"ch5_mall.csv\")\nmall = readr::read_csv(file) %>%\n  rename(Income = 4, Score = 5)\nmall## # A tibble: 200 × 5\n##    CustomerID Gender   Age Income Score\n##         <dbl> <chr>  <dbl>  <dbl> <dbl>\n##  1          1 Male      19     15    39\n##  2          2 Male      21     15    81\n##  3          3 Female    20     16     6\n##  4          4 Female    23     16    77\n##  5          5 Female    31     17    40\n##  6          6 Female    22     17    76\n##  7          7 Female    35     18     6\n##  8          8 Female    23     18    94\n##  9          9 Male      64     19     3\n## 10         10 Female    30     19    72\n## # … with 190 more rows\n## # ℹ Use `print(n = ...)` to see more rows\nlibrary(dplyr)\nstdmall = scale(mall[,3:5], center = TRUE, scale = TRUE)\nsummary(stdmall)##       Age              Income             Score          \n##  Min.   :-1.4926   Min.   :-1.73465   Min.   :-1.905240  \n##  1st Qu.:-0.7230   1st Qu.:-0.72569   1st Qu.:-0.598292  \n##  Median :-0.2040   Median : 0.03579   Median :-0.007745  \n##  Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.000000  \n##  3rd Qu.: 0.7266   3rd Qu.: 0.66401   3rd Qu.: 0.882916  \n##  Max.   : 2.2299   Max.   : 2.91037   Max.   : 1.889750"},{"path":"clustering-analysis.html","id":"k-means-聚类","chapter":"第 6 章 聚类分析","heading":"6.1.1 K-means 聚类","text":"使用 kmeans() 函数聚类，centers = 5 表示聚为 5 个类别，iter.max = 99 表示算法最多循环 99 次，nstart = 25 表示进行 25 次随机初始化，取目标函数值最小的聚类结果。聚类后得到一个 kmeans 类，其包含的 slot 如下：cluster：各个观测所属的类别；centers：各个类别的中心；totss：总平方和；tot.withinss：组内平方和；betweenss：组间平方和；size：各个类别的观测数。将聚类的结果附加到 mall 数据集中去。并按照客户年龄、年收入和消费得分进行可视化（图 6.1）。\n图 6.1: 使用 K-means 聚类后的结果\n","code":"\nmall.kmeans = kmeans(stdmall, centers = 5, iter.max = 99, nstart = 25)\nmall.kmeans## K-means clustering with 5 clusters of sizes 39, 47, 40, 20, 54\n## \n## Cluster means:\n##           Age     Income       Score\n## 1  0.07314728  0.9725047 -1.19429976\n## 2  1.20182469 -0.2351832 -0.05223672\n## 3 -0.42773261  0.9724070  1.21304137\n## 4  0.52974416 -1.2872781 -1.23337167\n## 5 -0.97822376 -0.7411999  0.46627028\n## \n## Clustering vector:\n##  [1] 5 5 4 5 5 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5\n##  [ reached getOption(\"max.print\") -- omitted 170 entries ]\n## \n## Within cluster sum of squares by cluster:\n## [1] 46.38992 26.65665 23.91544 18.58760 51.85673\n##  (between_SS / total_SS =  72.0 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nmall.kmeans.cluster = mall.kmeans$cluster\ntable(mall.kmeans.cluster)## mall.kmeans.cluster\n##  1  2  3  4  5 \n## 39 47 40 20 54\nmall.withcluster = mall %>%\n  mutate(kmeans_cluster = as.factor(mall.kmeans.cluster))\n\nlibrary(ggplot2)\np = ggplot(mall.withcluster, aes(color = kmeans_cluster)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(color = \"Cluster\")\n\np1 = p + aes(Age, Income)\np2 = p + aes(Age, Score)\np3 = p + aes(Income, Score)\n\nggpubr::ggarrange(p1, p2, p3, common.legend = TRUE, legend = \"right\")"},{"path":"clustering-analysis.html","id":"层次聚类法","chapter":"第 6 章 聚类分析","heading":"6.1.2 层次聚类法","text":"接下来使用层次聚类法重做上述任务，并可视化比较两者结果的差别。dist() 默认将计算观测值间的欧氏距离，method = \"average\" 指定使用平均连接法。plot() 可以直接画出聚类树。将层级聚类结果切分为 5 个类别。红色的方框叠加显示了各类别的划分。将层次聚类的结果加入原始数据集，并可视化。将两种聚类方法得到的结果放在一起对比，发现基本一致（图 6.2）。\n图 6.2: K-means 和 层次聚类结果的比较。\n","code":"\ntree = hclust(dist(stdmall), method = \"average\")\nmall.hclust.cluster = cutree(tree, k = 5)\nplot(tree)\nrect.hclust(tree, k = 5)\nmall.withcluster = mall.withcluster %>%\n  mutate(hclust_cluster = as.factor(mall.hclust.cluster))\n\np = ggplot(mall.withcluster, aes(color = hclust_cluster)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(color = \"Cluster\")\n\np4 = p + aes(Age, Income)\np5 = p + aes(Age, Score)\np6 = p + aes(Income, Score)\n\ncowplot::plot_grid(ggpubr::ggarrange(p1, p2, p3, common.legend = TRUE, legend = \"right\", ncol = 3),\n                   ggpubr::ggarrange( p4, p5, p6, common.legend = TRUE, legend = \"right\", ncol = 3),\n                   ncol = 1, labels = \"AUTO\")"},{"path":"clustering-analysis.html","id":"最佳类别数","chapter":"第 6 章 聚类分析","heading":"6.1.3 最佳类别数","text":"NbClust 软件包整合了判断最佳类别数的 30 种统计方法，从而能够综合各统计方法的结果得到最佳类别数。Best.partition 记录了综合各个统计指标所得的最佳分类结果。下面使用平均连接层次聚类，并将最佳结果也放到 mall.withcluster 中去。比较最佳类别数下的结果（图 6.3）。\n图 6.3: 使用 NbClust 计算得到的最佳类别数聚类后的结果。（）最佳类别数为 2 的 K-means 聚类；（B）最佳类别数为 2 的层次聚类。\n","code":"\nlibrary(NbClust)\nmall.nbclust.kmeans = NbClust(stdmall, method = \"kmeans\")## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## ## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 4 proposed 2 as the best number of clusters \n## * 3 proposed 3 as the best number of clusters \n## * 2 proposed 4 as the best number of clusters \n## * 4 proposed 5 as the best number of clusters \n## * 4 proposed 6 as the best number of clusters \n## * 1 proposed 9 as the best number of clusters \n## * 1 proposed 10 as the best number of clusters \n## * 2 proposed 12 as the best number of clusters \n## * 1 proposed 13 as the best number of clusters \n## * 1 proposed 15 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  2 \n##  \n##  \n## *******************************************************************\nmall.nbclust.kmeans## $All.index\n##         KL       CH Hartigan     CCC     Scott Marriot     TrCovW   TraceW\n## 2   4.4767 107.0956  63.1628  0.7428  315.5485 5809581 17735.8758 387.4393\n##    Friedman   Rubin Cindex     DB Silhouette   Duda Pseudot2   Beale Ratkowsky\n## 2    2.7054  1.5409 0.3385 1.3557     0.3355 1.1952 -25.6390 -0.2753    0.3427\n##        Ball Ptbiserial    Frey McClain   Dunn Hubert SDindex Dindex   SDbw\n## 2  193.7196     0.4907  0.4209  0.6628 0.0596 0.0029  3.0632 1.2926 1.9128\n##  [ reached getOption(\"max.print\") -- omitted 13 rows ]\n## \n## $All.CriticalValues\n##    CritValue_Duda CritValue_PseudoT2 Fvalue_Beale\n## 2          0.5679           119.4765       1.0000\n## 3          0.5171           126.0642       1.0000\n## 4          0.4304            95.2960       1.0000\n## 5          0.4622            77.9553       1.0000\n## 6          0.4551            46.7039       0.6456\n## 7          0.2298           214.5346       1.0000\n## 8          0.4050            60.2358       0.8577\n## 9          0.1434           119.4233       1.0000\n## 10         0.1148           146.5100       1.0000\n## 11         0.4304            48.9716       0.2292\n##  [ reached getOption(\"max.print\") -- omitted 4 rows ]\n## \n## $Best.nc\n##                      KL       CH Hartigan     CCC    Scott Marriot   TrCovW\n## Number_clusters 12.0000   6.0000   4.0000 12.0000   5.0000       5     4.00\n##                  TraceW Friedman   Rubin Cindex      DB Silhouette   Duda\n## Number_clusters  3.0000   5.0000  6.0000 9.0000 10.0000     6.0000 2.0000\n##                 PseudoT2   Beale Ratkowsky    Ball PtBiserial Frey McClain\n## Number_clusters    2.000  2.0000    3.0000  3.0000     5.0000    1  2.0000\n##                   Dunn Hubert SDindex Dindex    SDbw\n## Number_clusters 15.000      0  6.0000      0 13.0000\n##  [ reached getOption(\"max.print\") -- omitted 1 row ]\n## \n## $Best.partition\n##  [1] 2 2 1 2 2 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n##  [ reached getOption(\"max.print\") -- omitted 170 entries ]\nmall.withcluster = mall.withcluster %>%\n  mutate(best_kmeans_cluster = as.factor(mall.nbclust.kmeans$Best.partition))\nmall.nbclust.average = NbClust(stdmall, method = \"average\")## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## ## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 4 proposed 2 as the best number of clusters \n## * 3 proposed 3 as the best number of clusters \n## * 2 proposed 4 as the best number of clusters \n## * 3 proposed 5 as the best number of clusters \n## * 1 proposed 6 as the best number of clusters \n## * 1 proposed 8 as the best number of clusters \n## * 2 proposed 9 as the best number of clusters \n## * 1 proposed 11 as the best number of clusters \n## * 1 proposed 12 as the best number of clusters \n## * 3 proposed 13 as the best number of clusters \n## * 2 proposed 15 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  2 \n##  \n##  \n## *******************************************************************\nmall.withcluster$best_average_cluster = as.factor(mall.nbclust.average$Best.partition)\np = ggplot(mall.withcluster) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(color = \"Cluster\")\n\np7 = p + aes(Age, Income, color = best_kmeans_cluster)\np8 = p + aes(Age, Score, color = best_kmeans_cluster)\np9 = p + aes(Income, Score, color = best_kmeans_cluster)\np10 = p + aes(Age, Income, color = best_average_cluster)\np11 = p + aes(Age, Score, color = best_average_cluster)\np12 = p + aes(Income, Score, color = best_average_cluster)\n\ncowplot::plot_grid(ggpubr::ggarrange(p7, p8, p9, common.legend = TRUE, legend = \"right\", ncol = 3),\n                   ggpubr::ggarrange( p10, p11, p12, common.legend = TRUE, legend = \"right\", ncol = 3),\n                   ncol = 1, labels = \"AUTO\")"},{"path":"linear-model.html","id":"linear-model","chapter":"第 7 章 线性模型和广义线性模型","heading":"第 7 章 线性模型和广义线性模型","text":"线性模型是实际应用中最常用的统计模型。","code":""},{"path":"linear-model.html","id":"模型假设","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.1 模型假设","text":"","code":""},{"path":"linear-model.html","id":"线性模型假设","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.1.1 线性模型假设","text":"经典线性模型的应用是有前提条件的。自变量 \\(x = (x_1, ..., x_p)^T\\) 被看做是给定的，而因变量 \\(Y\\) 来自均值为 \\(μ\\)，方差为 \\(σ^2\\) 的正态分布 \\(N(μ, σ^2)\\)，其中 \\(μ\\) 与 自变量 \\(x\\) 的关系是：\\[μ = \\alpha + x^T\\beta = \\alpha + \\beta_1x_1 + ... + \\beta_px_p\\]这里回归系数 \\(\\alpha\\) 是截距项，回归系数 \\(\\beta = (\\beta_1, ..., \\beta_p)^T\\) 是对自变量的斜率。因为线性模型的假设，所以在应用的时候需要进行模型诊断，才能决定适用性。模型诊断需要注意的地方包括：标准化残差图异常点自相关性多重共现性","code":""},{"path":"linear-model.html","id":"广义线性模型假设","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.1.2 广义线性模型假设","text":"广义线性模型则推广了线性模型，适用于因变量是定类变量、定序变量等的情形。广义线性模型有 3 个成分，分别是随机成分、系统成分、连接函数。","code":""},{"path":"linear-model.html","id":"线性模型预测房屋价格","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.2 线性模型预测房屋价格","text":"ch6_house.csv 数据集记录了某地区 21613 座房屋的 10 个变量信息，我们据此可以建立线性模型预测房屋价格。house 数据集的 10 个变量的定义如下：price：房屋价格bedrooms：卧室数目bathrooms：卫生间数目sqft_living：住房面积（平方英尺）sqft_lot：房基地面积floors：楼层数目condition：房屋整体状况的好坏，取值 1 - 5grade：房屋等级，取值 1 - 13sqft_above：除地下室以外的住房面积yr_built：房屋建成年份（最早 1900，最晚 2015）房屋价格变量本身是不符合正态分布的，将其进行转换后符合正态分布。（为什么？）\n图 7.1: 转换前后房屋价格的直方图和 QQ 图\n","code":"\nlibrary(readr)\nfile = xfun::magic_path(\"ch6_house.csv\")\nhouse = read_csv(file)\nhouse## # A tibble: 21,613 × 10\n##      price bedrooms bathr…¹ sqft_…² sqft_…³ floors condi…⁴ grade sqft_…⁵ yr_bu…⁶\n##      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl> <dbl>   <dbl>   <dbl>\n##  1  221900        3    1       1180    5650      1       3     7    1180    1955\n##  2  538000        3    2.25    2570    7242      2       3     7    2170    1951\n##  3  180000        2    1        770   10000      1       3     6     770    1933\n##  4  604000        4    3       1960    5000      1       5     7    1050    1965\n##  5  510000        3    2       1680    8080      1       3     8    1680    1987\n##  6 1230000        4    4.5     5420  101930      1       3    11    3890    2001\n##  7  257500        3    2.25    1715    6819      2       3     7    1715    1995\n##  8  291850        3    1.5     1060    9711      1       3     7    1060    1963\n##  9  229500        3    1       1780    7470      1       3     7    1050    1960\n## 10  323000        3    2.5     1890    6560      2       3     7    1890    2003\n## # … with 21,603 more rows, and abbreviated variable names ¹​bathrooms,\n## #   ²​sqft_living, ³​sqft_lot, ⁴​condition, ⁵​sqft_above, ⁶​yr_built\n## # ℹ Use `print(n = ...)` to see more rows\npar(mfrow = c(2,2))\nhist(house$price)\nqqnorm(house$price)\nhist(log(house$price))\nqqnorm(log(house$price))"},{"path":"linear-model.html","id":"对房屋数据进行标准化","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.2.1 对房屋数据进行标准化","text":"将 price、sqft_living 等变量均取对数。同时，因为时间是无限增长的，在建模数据集中出现的时间不同于预测数据集中出现的时间，所以时间无法直接应用于建模。必须对时间变量进行转换。将数据集随机划分为学习数据集和测试数据集。","code":"\nlibrary(dplyr)\nvarname = c(\"price\", \"sqft_living\", \"sqft_lot\", \"sqft_above\")\nhouse = house %>%\n  mutate(across(varname, log)) %>%\n  rename_with(~ paste0(\"log_\",.x), .cols = varname) %>%\n  mutate(age = 2015 - yr_built) %>% # 对年份进行转换\n  select(-yr_built)\nset.seed(12345)\nid_learning = sample(1:nrow(house), round(0.7 * nrow(house)))\nhouse_learning = house[id_learning,]\nhouse_test = house[-id_learning,]"},{"path":"linear-model.html","id":"拟合线性模型","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.2.2 拟合线性模型","text":"对学习数据集建立线性模型。使用 summary() 查看模型系数的估计、残差的统计量、R 方等信息。从输出结果看，模型各自变量的贡献均比较显著，R 方值为 0.64。提取模型的系数估计值。提取模型的系数置信区间，level = 0.95 表示提取 95% 置信区间。提取模型的因变量拟合值和残差。","code":"\nfit.lm = lm(log_price ~ ., data = house_learning)\nsummary(fit.lm)## \n## Call:\n## lm(formula = log_price ~ ., data = house_learning)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.15983 -0.20963  0.01033  0.20339  1.65555 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      7.8340456  0.0732372 106.968  < 2e-16 ***\n## bedrooms        -0.0433545  0.0035877 -12.084  < 2e-16 ***\n## bathrooms        0.0813344  0.0059163  13.748  < 2e-16 ***\n## log_sqft_living  0.4887105  0.0160515  30.446  < 2e-16 ***\n## log_sqft_lot    -0.0332896  0.0034989  -9.514  < 2e-16 ***\n## floors           0.0721198  0.0072678   9.923  < 2e-16 ***\n##  [ reached getOption(\"max.print\") -- omitted 4 rows ]\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3155 on 15119 degrees of freedom\n## Multiple R-squared:  0.6406, Adjusted R-squared:  0.6404 \n## F-statistic:  2994 on 9 and 15119 DF,  p-value: < 2.2e-16\ncoefficients(fit.lm)##     (Intercept)        bedrooms       bathrooms log_sqft_living    log_sqft_lot \n##     7.834045552    -0.043354513     0.081334366     0.488710493    -0.033289648 \n##          floors       condition           grade  log_sqft_above             age \n##     0.072119815     0.040946870     0.241517661    -0.076794423     0.006058074\nconfint(fit.lm, level = 0.95)##                        2.5 %       97.5 %\n## (Intercept)      7.690491718  7.977599387\n## bedrooms        -0.050386772 -0.036322253\n## bathrooms        0.069737781  0.092930951\n## log_sqft_living  0.457247630  0.520173356\n## log_sqft_lot    -0.040147996 -0.026431301\n## floors           0.057873956  0.086365675\n## condition        0.032523068  0.049370673\n## grade            0.234395589  0.248639733\n## log_sqft_above  -0.106385018 -0.047203827\n## age              0.005833631  0.006282517\nyhat = fitted(fit.lm)\nresid = residuals(fit.lm)"},{"path":"linear-model.html","id":"线性模型的模型诊断","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.2.3 线性模型的模型诊断","text":"图 7.2 中：左上是残差对拟合值的散点图。图中黑线表明，除了拟合值比较小的少部分数据点，残差的平均值接近于零，因而满足线性假设；对于不同的拟合值，残差围绕平均值变化的范围相当，因而满足同方差假设。右上是标准化残差的正态 QQ 图。可以看出残差大致符合正态分布，但也有少数异常值偏离较大。左下是标准化残差绝对值的平方根对拟合值的散点图，也可用于更方便的检查同方差假设是否成立。在本例中，大部分的数据点拟合值落在 [12, 15] 区间，在这一区间同方差假设是成立的（黑线水平，且黑线上下点的变化范围接近）。右下是各观测的 Cook 距离图。从中可见，学习数据中的 5710 号观测点是异常点。\n图 7.2: 对线性模型进行诊断。\n","code":"\npar(mfrow = c(2, 2),\n    mar = c(2.5, 2.5, 1.5, 1.5))\nplot(fit.lm, which = 1:4)"},{"path":"linear-model.html","id":"优化线性模型","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.2.4 优化线性模型","text":"将异常点去掉，重新拟合线性模型。异常值就没有那么大了（图 7.3）。\n图 7.3: 对去除 1 个异常值后的新线性模型进行诊断。\n使用所得的线性模型对测试数据集进行预测，计算均方根误差，查看预测价格与实际价格的偏差。","code":"\nfit2.lm = lm(log_price ~ ., data = house_learning[-5710,])\npar(mfrow = c(2, 2),\n    mar = c(2.5, 2.5, 1.5, 1.5))\nplot(fit2.lm, which = 1:4)\npred.lm = predict(fit2.lm, house_test)\n\n# 均方根误差\nrmse.lm = sqrt(mean((exp(pred.lm)-exp(house_test$log_price))^2))\n\n# x 轴预测价格，y 轴实际价格\nplot(exp(pred.lm), exp(house_test$log_price)) \nabline(a = 0, b = 1, col = \"red\")"},{"path":"linear-model.html","id":"逻辑回归预测是否患糖尿病","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.3 逻辑回归预测是否患糖尿病","text":"ch6_diabetes.csv 数据集记录了 768 位印第安女性的糖尿病患病资料。Pregnacies：怀孕次数Glucose：餐后 2 h 血糖BloodPressure：舒张压（mmHg）SkinThickness：肱三头肌皮褶厚度（mm）Insulin：餐后 2 h 的胰岛素水平BMI：体重指数DiabetesPredigreeFunction：糖尿病谱系功能Age：年龄Outcome：因变量，1 表示有糖尿病，0 表示不患病。","code":"\nfile = xfun::magic_path(\"ch6_diabetes.csv\")\ndiabetes = read_csv(file)\ndiabetes## # A tibble: 768 × 9\n##    Pregnancies Glucose BloodPressure SkinT…¹ Insulin   BMI Diabe…²   Age Outcome\n##          <dbl>   <dbl>         <dbl>   <dbl>   <dbl> <dbl>   <dbl> <dbl>   <dbl>\n##  1           6     148            72      35       0  33.6   0.627    50       1\n##  2           1      85            66      29       0  26.6   0.351    31       0\n##  3           8     183            64       0       0  23.3   0.672    32       1\n##  4           1      89            66      23      94  28.1   0.167    21       0\n##  5           0     137            40      35     168  43.1   2.29     33       1\n##  6           5     116            74       0       0  25.6   0.201    30       0\n##  7           3      78            50      32      88  31     0.248    26       1\n##  8          10     115             0       0       0  35.3   0.134    29       0\n##  9           2     197            70      45     543  30.5   0.158    53       1\n## 10           8     125            96       0       0   0     0.232    54       1\n## # … with 758 more rows, and abbreviated variable names ¹​SkinThickness,\n## #   ²​DiabetesPedigreeFunction\n## # ℹ Use `print(n = ...)` to see more rows"},{"path":"linear-model.html","id":"分层拆分糖尿病患病数据集","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.3.1 分层拆分糖尿病患病数据集","text":"sampling(Tillé Matei 2021) 包有各种抽样函数，可用于分层抽样将数据集分为学习数据集和测试数据集。这里使用 strata() 函数分层取 70% 的数据作为学习数据集。参数 stratanames 指定了分层变量的名字；参数 size 给出每层随机抽取的观测数；参数 method = \"srswor\" 说明在每层中使用无放回的简单随机抽样。learning_sample$ID_unit 给出了抽样得到的结果，可用于获取拆分后的数据集。","code":"\nset.seed(12345)\nlibrary(sampling)\n\ndiabetes = diabetes %>% # 分层抽样需要将分层变量排序后才能进行\n  arrange(Outcome)\n\nlearning_sample = strata(diabetes, stratanames = (\"Outcome\"),\n                         size = round(0.7 * table(diabetes$Outcome)),\n                         method = \"srswor\")\n\n# 学习数据集\ndiabetes_learning = diabetes[learning_sample$ID_unit,]\n\n# 测试数据集\ndiabetes_test = diabetes[-learning_sample$ID_unit,]"},{"path":"linear-model.html","id":"模型拟合和验证","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.3.2 模型拟合和验证","text":"使用 glm() 拟合广义线性模型。family = \"binomial\" 指定了连接函数的类型（因变量分布为二项分布），从而得到一个 Logit 模型。将 Logit 模型应用于测试数据集对因变量进行预测。type = \"response\" 指定预测值为因变量取 1 的概率，并使用概率是否大于 0.5 为分界线，预测因变量类别为 1 或 0。查看因变量真实值域预测值的列联表。真实值为 0（未患病）的有 134 例被预测为 0，16 例被预测为 1；真实值为 1（患病）的有 36 例被预测为 0，44 例被预测为 1。整个模型的预测准确性堪忧。使用 LASSO 算法3 进行变量选择可以得到最佳模型。因为最佳模型与原模型的效果差别不大，所以不再赘述。","code":"\nfit.logit = glm(Outcome ~ ., data = diabetes_learning,\n                family = \"binomial\")\nsummary(fit.logit)## \n## Call:\n## glm(formula = Outcome ~ ., family = \"binomial\", data = diabetes_learning)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.3442  -0.7207  -0.4096   0.7434   2.2946  \n## \n## Coefficients:\n##                           Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)              -8.289674   0.848811  -9.766  < 2e-16 ***\n## Pregnancies               0.124052   0.038682   3.207  0.00134 ** \n## Glucose                   0.037027   0.004547   8.144 3.83e-16 ***\n## BloodPressure            -0.017535   0.006206  -2.825  0.00472 ** \n## SkinThickness             0.005478   0.008023   0.683  0.49476    \n## Insulin                  -0.001491   0.001046  -1.426  0.15389    \n##  [ reached getOption(\"max.print\") -- omitted 3 rows ]\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 696.28  on 537  degrees of freedom\n## Residual deviance: 505.64  on 529  degrees of freedom\n## AIC: 523.64\n## \n## Number of Fisher Scoring iterations: 5\ntest.pred.logit = 1 * (predict(fit.logit, diabetes_test, type = \"response\") > 0.5)\ntable(diabetes_test$Outcome, test.pred.logit)##    test.pred.logit\n##       0   1\n##   0 134  16\n##   1  36  44"},{"path":"linear-model.html","id":"逻辑回归分析预测手机用户流失","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.4 逻辑回归分析预测手机用户流失","text":"本部分使用的移动运营商数据有多个数据集构成，分别记录了某移动运营商流失客户的信息和使用行为，以及未流失客户的信息和使用行为。","code":""},{"path":"linear-model.html","id":"读取预处理好的数据集","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.4.1 读取预处理好的数据集","text":"在进行广义线性模型回归分析前，已经对数据进行了分层抽样，并将建模数据集分为学习数据集和测试数据集，并在学习数据集中通过欠抽样抽取了 10 个样本数据集 4。","code":"\n# 读取 10 个学习数据集\nlearn = vector(mode = \"list\", 10)\nfor (k in 1:10){\n  file = xfun::magic_path(paste0(\"ch3_mobile_learning_sample\", k, \"_imputed.csv\"))\n  learn[[k]] = read_csv(file, locale = locale(encoding = \"GB2312\")) %>%\n    arrange(`设备编码`)\n}\n\n# 读取 10 个测试数据集\ntest = lapply(1:10, function(k){\n  paste0(\"ch3_mobile_test_sample\", k, \"_imputed.csv\") %>%\n    xfun::magic_path() %>%\n    read_csv(locale = locale(encoding = \"GB2312\")) %>%\n    arrange(`设备编码`)\n})"},{"path":"linear-model.html","id":"建立均值逻辑回归模型","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.4.2 建立（均值）逻辑回归模型","text":"根据学习数据集的每个插补后样本数据集建立逻辑模型。使用去掉 设备编码 后的数据集为输入，以 是否流失 为因变量，其它变量为自变量，family = \"binomial\" 指定因变量分布为二项分布。将根据学习数据集的 10 个插补后数据集分别建立模型所得的 10 组预测流失概率进行平均，得到测试数据集的预测流失概率。","code":"\nlibrary(glmnet)\nfit.logit = lapply(1:10, function(k){\n  # 逻辑回归模型\n   glm( `是否流失` ~ ., \n                   data = learn[[k]] %>% select(-`设备编码`),\n                   family = \"binomial\")\n})\nprob.logit.set = lapply(1:10, function(k){\n  # 将模型应用于相应的测试数据集\n  predict(fit.logit[[k]], test[[k]], type = \"response\")\n})\nprob.logit = bind_rows(prob.logit.set) %>% colMeans()"},{"path":"linear-model.html","id":"计算逻辑回归模型预测的准确率","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.4.3 计算逻辑回归模型预测的准确率","text":"生成列联表后，分别计算模型预测的准确率。","code":"\nclass.logit = 1 * (prob.logit > 0.5)\nconmat.logit = table(test[[1]][[\"是否流失\"]], class.logit)\nconmat.logit##    class.logit\n##        0    1\n##   0 4921  930\n##   1   20  136\n# 未流失用户被正确预测的比例\naccu.y0.logit = conmat.logit[1, 1]/sum(conmat.logit[1, ])\n\n# 流失用户被正确预测的比例\naccu.y1.logit = conmat.logit[2, 2]/sum(conmat.logit[2, ])\n\n# 所有用户被正确预测的比例\naccu.logit = sum(diag(conmat.logit))/sum(conmat.logit) # 使用 diag() 取矩阵的对角线数值"},{"path":"linear-model.html","id":"lasso-分析预测手机用户流失","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.5 LASSO 分析预测手机用户流失","text":"接下来使用 LASSO 分析进行手机用户流失预测，并比较 LASSO 分析得出的模型与逻辑回归模型的准确性。注意：LASSO 本身并不等价于线性模型，而是在针对多个变量建模时，为了简化线性模型，用来帮助选择线性模型变量的方法。","code":""},{"path":"linear-model.html","id":"使用学习数据集建立-lasso-回归模型","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.5.1 使用学习数据集建立 LASSO 回归模型","text":"使用 10 个运营商数据插补生成的学习数据集进行建模。cv.glmnet() 对 glmnet 模型进行多重交叉验证，然后返回一个 cv.glmnet 类。该类不仅包括预测模型，还包括交叉验证中计算得到的多个成分。将模型应用于相应的插补后测试数据集进行预测，并计算 LASSO 模型的回归系数。","code":"\ncvfit.lasso = lapply(1:10, function(k){\n  # 只使用学习数据集中的自变量矩阵\n  x_learn = as.matrix(learn[[k]][, 2:58])\n  \n  # 使用交叉验证选出调节参数 lambda 的最佳值\n  cv.glmnet(x_learn, learn[[k]][['是否流失']],\n            family = \"binomial\",\n            type.measure = \"class\")\n})\nprob.lasso = lapply(1:10, function(k){\n  x_test = as.matrix(test[[k]][, 2:58])\n  predict(cvfit.lasso[[k]], x_test,\n          s = 'lambda.min',\n          type = \"response\")\n})\n\ncoef.lasso = lapply(1:10, function(k){\n  as.matrix(coef(cvfit.lasso[[k]], s = 'lambda.min'))[, 1]\n})"},{"path":"linear-model.html","id":"计算-lasso-模型的预测准确率","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.5.2 计算 LASSO 模型的预测准确率","text":"计算 10 个 LASSO 模型平均的预测流失概率，并以概率 0.5 为阈值计算模型的预测准确率。分别计算不同类型的预测准确度。","code":"\nprob.lasso.mean = do.call(\"cbind\", prob.lasso) %>%\n  rowMeans()\n\nclass.lasso = 1 * (prob.lasso.mean > 0.5)\nconmat.lasso = table(test[[1]][[\"是否流失\"]], class.lasso)\nconmat.lasso##    class.lasso\n##        0    1\n##   0 4919  932\n##   1   18  138\n# 未流失用户被正确预测的比例\naccu.y0.lasso = conmat.lasso[1, 1]/sum(conmat.lasso[1, ])\n\n# 流失用户被正确预测的比例\naccu.y1.lasso = conmat.lasso[2, 2]/sum(conmat.lasso[2, ])\n\n# 所有用户被正确预测的比例\naccu.lasso = sum(diag(conmat.lasso))/sum(conmat.lasso) # 使用 diag() 取矩阵的对角线数值"},{"path":"linear-model.html","id":"比较逻辑回归模型和-lasso-模型","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.6 比较逻辑回归模型和 LASSO 模型","text":"上面针对同样的运营商数据，分别建立了逻辑回归模型和 LASSO 模型，这两种模型得到的结果会有什么不同呢？接下来将回答这个问题。","code":""},{"path":"linear-model.html","id":"模型预测准确率的差异","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.6.1 模型预测准确率的差异","text":"两种预测模型的准确率差异并不大。","code":"\naccu = tibble(\n  type = c(\"未流失用户准确预测比例\", \"流失用户准确预测比例\", \"所有用户准确预测比例\"),\n  logit = c(accu.y0.logit, accu.y1.logit, accu.logit),\n  lasso = c(accu.y0.lasso, accu.y1.lasso, accu.lasso)\n)\naccu## # A tibble: 3 × 3\n##   type                   logit lasso\n##   <chr>                  <dbl> <dbl>\n## 1 未流失用户准确预测比例 0.841 0.841\n## 2 流失用户准确预测比例   0.872 0.885\n## 3 所有用户准确预测比例   0.842 0.842"},{"path":"linear-model.html","id":"模型使用的自变量","chapter":"第 7 章 线性模型和广义线性模型","heading":"7.6.2 模型使用的自变量","text":"根据方法的原理，可以得出 LASSO 模型使用的自变量可能会比逻辑模型要少一些。因此，下面分别分析每个 LASSO 模型纳入的自变量。因为运营商数据包含的数据维度多达 57 个，所以在这些模型中实际上纳入的自变量可能是不同的，并且即便是两个模型纳入了同一个自变量，其在模型中的作用也可能是不同的。为了展示这种现象，这里将自变量系数区分为 3 类，大于 0，小于 0 和等于 0。可以发现：在 10 个模型中，均为使用的变量就有 8 个，另外有其它变量被模型使用的次数在 1 - 9 之间，而被 10 个模型均纳入的变量只有 6 个。超过 5 个模型选用的自变量有约 17 个，它们被不同模型使用的情况如下所示。实际上，即便不使用 LASSO 分析，在广义线性模型中也会报告有哪些自变量在模型中占有更高的权重。summary() 输出结果中的最后一列（Pr(>|z|）中，凡是小于 0.05 的都被特别标注了出来，这些便是模型中的主要参量。","code":"\ncoef.indic.lasso = lapply(1:10, function(k){\n  coef.lasso[[k]][-1] %>%  # 去掉截距项\n    t() %>%\n    as_tibble()\n}) %>%\n  bind_rows() %>%\n  mutate_all( .funs = function(x) ifelse(x == 0, 0, ifelse(x > 0, 1, -1)))\ncolSums(abs(coef.indic.lasso)) %>% table() %>%\n  barplot()\nlibrary(pheatmap)\nlibrary(RColorBrewer)\nidx = which(colSums(abs(coef.indic.lasso)) > 5)\npheatmap(t(coef.indic.lasso[, idx]), \n         show_rownames = TRUE, \n         show_colnames = FALSE, \n         color = colorRampPalette(rev(brewer.pal(n = 7, name = \"RdBu\")))(100),\n         breaks = seq(-1, 1, length.out = 100))\nsummary(fit.logit[[1]])## \n## Call:\n## glm(formula = 是否流失 ~ ., family = \"binomial\", data = learn[[k]] %>% \n##     select(-设备编码))\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -3.15546  -0.38758  -0.00667   0.39992   2.87340  \n## \n## Coefficients:\n##                      Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)          15.57839   12.23879   1.273  0.20306    \n## 彩铃费                1.19525    3.18991   0.375  0.70789    \n## 短信费               -0.40472    0.71838  -0.563  0.57318    \n## 本地语音通话费       -0.05480    0.31305  -0.175  0.86105    \n## 长途语音通话费        0.15550    0.40991   0.379  0.70442    \n## 省内语音漫游费        0.77114    0.38576   1.999  0.04561 *  \n##  [ reached getOption(\"max.print\") -- omitted 52 rows ]\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1009.22  on 727  degrees of freedom\n## Residual deviance:  434.86  on 670  degrees of freedom\n## AIC: 550.86\n## \n## Number of Fisher Scoring iterations: 8"},{"path":"neural-network.html","id":"neural-network","chapter":"第 8 章 神经网络","heading":"第 8 章 神经网络","text":"","code":""},{"path":"neural-network.html","id":"concept-of-neural-network","chapter":"第 8 章 神经网络","heading":"8.1 神经网络的基本概念","text":"","code":""},{"path":"neural-network.html","id":"神经元","chapter":"第 8 章 神经网络","heading":"8.1.1 神经元","text":"神经元是神经网络的基本单元。一个神经元由输入端、组合函数、激活函数、输出端组成。组合函数和激活函数是神经网络的核心。\n图 8.1: 神经元的结构\n人工神经元是一个基于生物神经元的数学模型，神经元接受多个输入信息，对它们进行加权求和，再经过一个激活函数处理，然后将这个结果输出。对于神经网络而言，神经元节点本身相当于一个神经细胞，输入相当于树突，带权重的连接相当于轴突，输出相当于突触5。","code":"\nknitr::include_graphics(\"https://vnote-1251564393.cos.ap-chengdu.myqcloud.com/typora-img/20220420092837.png\")"},{"path":"neural-network.html","id":"多层感知器","chapter":"第 8 章 神经网络","heading":"8.1.2 多层感知器","text":"多个神经元连接在一起就形成了神经网络。多层感知器是一种常用的神经网络。各个自变量通过输入层的神经元输入到网络，输入层的输出传递给隐藏层，作为后者的输入；数据经过多个隐藏层的传递后，最终被转换后的数据在输出层形成输出值。多层感知器通常在隐藏层使用线性组合函数和 S 型激活函数，在输出层使用线性组合函数和与因变量相适应的激活函数。多层感知器可以形成很复杂的非线性模型。只要给予足够的数据、隐藏层和训练时间，含一层隐藏层的多层感知器就能够以任意精确度近似自变量和因变量之间几乎任何形式的函数；使用多个隐藏层可能用更少的隐藏神经元和参数就能形成复杂的非线性模型，提高模型的泛化能力6。除了多层感知器，常见的神经网络还包括：卷积神经网络、循环神经网络等。卷积神经网络（CNN）旨在解决图像识别问题，卷积神经网络在图像识别、推荐系统以及自然语言处理等方面有着广泛的应用。循环神经网络在语音识别、生成图像描述、音乐合成和机器翻译等领域有着广泛的应用。不同类型的神经网络都是由多个神经元连接而成的，其主要区别在于神经元的连接规则不同。对于多层感知器而言，其连接规则为：神经元按照层来布局。最左边的输入层，负责接收输入数据；最右边的输出层，负责输出数据。中间是隐藏层，对于外部不可见。隐藏层可以包含多层，大于一层的就被称为深度神经网络，层次越多数据处理能力越强。同一层的神经元之间没有连接。前后两层的所有神经元相连，前层神经元的输出就是后层神经元的输入。每个连接都有一个权值。","code":"\nknitr::include_graphics(\"https://vnote-1251564393.cos.ap-chengdu.myqcloud.com/typora-img/20220420100007.png\")"},{"path":"neural-network.html","id":"组合函数和激活函数","chapter":"第 8 章 神经网络","heading":"8.1.3 组合函数和激活函数","text":"组合函数通常用线性组合函数，其实就是一个简单的按权重加和。\\[u_j = \\sum(v_1, ..., v_s) = b_j + \\sum_{r=1}^sw_{rj}v_r\\]激活函数是非线性函数，对组合函数的结果进行处理。它的可选类型比较多，主要有：S 型函数\nLogistic 函数：\\(y = \\frac{1}{1+e^{-x}} \\(0, 1)\\)\nTanh 函数（双曲正切函数）：\\(y = 1 - \\frac{2}{1+e^{2x}} \\(-1, 1)\\)\nEliot 函数（Softsign 函数）：\\(y = \\frac{x}{1+|x|} \\(-1, 1)\\)\nArctan 函数：\\(y = \\frac{2}{\\pi}arctan(x) \\(-1,1)\\)\nS 型函数Logistic 函数：\\(y = \\frac{1}{1+e^{-x}} \\(0, 1)\\)Tanh 函数（双曲正切函数）：\\(y = 1 - \\frac{2}{1+e^{2x}} \\(-1, 1)\\)Eliot 函数（Softsign 函数）：\\(y = \\frac{x}{1+|x|} \\(-1, 1)\\)Arctan 函数：\\(y = \\frac{2}{\\pi}arctan(x) \\(-1,1)\\)ReLU 函数（线性整流函数）：(8.1)ReLU 函数（线性整流函数）：(8.1)\\[\\begin{equation}\nf(x) =\n\\begin{cases}\nx  & \\text{} x \\geq 0 \\\\\n0  & \\text{} x < 0\n\\end{cases}\n\\tag{8.1}\n\\end{equation}\\]Softmax 函数：\\(y_j = \\frac{e^{u_j}}{\\sum_{j'=1}^je^{u_{j'}}} \\(0,1)\\)与 S 型函数和 ReLu 函数不同，Softmax 函数是多变量输入激活函数。Softmax 与正常的 max 函数不同：max 函数仅输出最大值，但 Softmax 确保较小的值具有较小的概率，并且不会直接丢弃。这些激活函数都能将组合函数产生的 \\((-\\infty, \\infty)\\) 通过单调连续的非线性转换变成有限的输出值。每种函数在运算速度、可微性、输出值等方面存在差异，因此具有不同的应用场景。","code":""},{"path":"neural-network.html","id":"神经网络的训练","chapter":"第 8 章 神经网络","heading":"8.1.4 神经网络的训练","text":"神经网络的训练，就是求解组合函数权重的过程。简单来说就是从基于误差函数，对权重值不断进行修正，最终是误差逐渐趋近为 0 的过程。误差函数越小，模型拟合效果越好。根据因变量的取值类型，要在输出层选用不同的激活函数。因变量是二值变量或比例，输出层激活函数采用 Logistic 函数；因变量是多种取值的定类变量，输出层激活函数使用 Softmax 函数或 Logistic 函数；因变量是多种取值的定序变量，可将其看做定类变量，或者根据多个输出单元的结果进行定序；因变量为计数变量（事件发生的次数），输出层的激活函数采用指数函数；因变量为取值可正可负的连续变量（如满足正态分布的数值），输出层激活函数采用恒等函数；因变量为非负连续变量（如收入、销售额），通常将因变量进行 Box-Cox 转换后，在使用因变量可正可负的方法。这部分内容解释起来比较复杂，暂且略过","code":""},{"path":"neural-network.html","id":"使用神经网络预测红酒品质","chapter":"第 8 章 神经网络","heading":"8.2 使用神经网络预测红酒品质","text":"ch7_wine.csv 记录了与红酒品质相关的 12 个变量，分别是：fixed.acidity：固定酸度volatile.acidity：挥发性酸度citric.acid：柠檬酸residual.sugar：残留的糖分chlorides：氯化物free.sulfur.dioxide：游离二氧化硫total.sulfur.dioxide：总二氧化硫density：密度pH：酸碱度sulphates：硫酸盐alcohol：酒精度quality：因变量，品质等级，取值 3 - 9。","code":"\nfile = xfun::magic_path(\"ch7_wine.csv\")\nwine = readr::read_csv(file)\nwine## # A tibble: 4,898 × 12\n##    fixed…¹ volat…² citri…³ resid…⁴ chlor…⁵ free.…⁶ total…⁷ density    pH sulph…⁸\n##      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl>\n##  1     7      0.27    0.36    20.7   0.045      45     170   1.00   3       0.45\n##  2     6.3    0.3     0.34     1.6   0.049      14     132   0.994  3.3     0.49\n##  3     8.1    0.28    0.4      6.9   0.05       30      97   0.995  3.26    0.44\n##  4     7.2    0.23    0.32     8.5   0.058      47     186   0.996  3.19    0.4 \n##  5     7.2    0.23    0.32     8.5   0.058      47     186   0.996  3.19    0.4 \n##  6     8.1    0.28    0.4      6.9   0.05       30      97   0.995  3.26    0.44\n##  7     6.2    0.32    0.16     7     0.045      30     136   0.995  3.18    0.47\n##  8     7      0.27    0.36    20.7   0.045      45     170   1.00   3       0.45\n##  9     6.3    0.3     0.34     1.6   0.049      14     132   0.994  3.3     0.49\n## 10     8.1    0.22    0.43     1.5   0.044      28     129   0.994  3.22    0.45\n## # … with 4,888 more rows, 2 more variables: alcohol <dbl>, quality <dbl>, and\n## #   abbreviated variable names ¹​fixed.acidity, ²​volatile.acidity, ³​citric.acid,\n## #   ⁴​residual.sugar, ⁵​chlorides, ⁶​free.sulfur.dioxide, ⁷​total.sulfur.dioxide,\n## #   ⁸​sulphates\n## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names"},{"path":"neural-network.html","id":"数据标准化和拆分","chapter":"第 8 章 神经网络","heading":"8.2.1 数据标准化和拆分","text":"将数据集标准化，并采用分层抽样的方法将其分为学习数据集和测试数据集。生成数据集的自变量矩阵。生成数据集的因变量矩阵。因变量矩阵分别指示定类变量和定序变量。因为因变量是定序变量，所以可以生成一个定序变量矩阵。在这里，如果一个观测的 quality 取值为 0 时，相应行的取值是 \\((1, 0, 0, 0, 0, 0, 0)\\)；如果取值是 6 时，相应行的取值是 \\((1, 1, 1, 1, 1, 1, 1, 1)\\)。同样生成测试数据集的相应结果。","code":"\nlibrary(dplyr)\nlibrary(sampling)\nset.seed(12345)\n\nwine = wine %>%\n  mutate_at(vars(-quality), scale) %>%\n  mutate(quality = quality - 3) %>%\n  arrange(quality)\n\ntrain_sample = strata(wine, stratanames = \"quality\",\n                      size = round(0.7 * table(wine$quality)),\n                      method = \"srswor\")\n\nwine_train = wine[train_sample$ID_unit,]\nwine_test = wine[-train_sample$ID_unit,]\nx_train = wine_train[,1:11] %>% as.matrix()\nx_test = wine_test[,1:11] %>% as.matrix()\nlibrary(keras)\ny_train.nom = to_categorical(wine_train$quality)\ntail(y_train.nom)##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [3424,]    0    0    0    0    0    1    0\n## [3425,]    0    0    0    0    0    1    0\n## [3426,]    0    0    0    0    0    0    1\n## [3427,]    0    0    0    0    0    0    1\n##  [ reached getOption(\"max.print\") -- omitted 2 rows ]\ny_train.ord = y_train.nom\n\nfor (i in 1:nrow(y_train.ord)){\n  j = which(y_train.ord[i,] == 1)\n  y_train.ord[i,1:j] = 1\n}\n\ntail(y_train.ord)##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [3424,]    1    1    1    1    1    1    0\n## [3425,]    1    1    1    1    1    1    0\n## [3426,]    1    1    1    1    1    1    1\n## [3427,]    1    1    1    1    1    1    1\n##  [ reached getOption(\"max.print\") -- omitted 2 rows ]\ny_test.nom = to_categorical(wine_test$quality)\n\ny_test.ord = y_test.nom\n\nfor (i in 1:nrow(y_test.ord)){\n  j = which(y_test.ord[i,] == 1)\n  y_train.ord[i, 1:j] = 1\n}"},{"path":"neural-network.html","id":"使用-tensorflow-神经网络","chapter":"第 8 章 神经网络","heading":"8.2.2 使用 TensorFlow 神经网络","text":"构建神经网络需要先配置模型的层，然后再编译模型。神经网络的基本组成部分是层。大多数深度学习都包括将简单的层链接在一起。大多数层（如 layer_Dense() 指定的层）都具有在训练期间才会学习的参数。该网络的第一层 layer_flatten() 将输入数据转换成一维数组（这里是 11 维的向量，如果是 28 * 28 的矩阵，则可以写 input_shape = c(28, 28)）。该层没有要学习的参数，它只会重新格式化数据。接下来是两个密集连接层或全连接层。第一个 Dense 层有 128 个节点（或神经元），第二个（也是最后一个）层会返回一个长度为 7 的数组。每个节点都包含一个得分，用来表示当前输入属于 7 个类别中的哪一类。","code":"\nmodel = keras_model_sequential() %>%\n  layer_flatten(input_shape = 11) %>%\n  layer_dense(units = 128, activation = \"relu\")  %>%\n  # layer_dropout(0.2) %>%\n  layer_dense(units = 7, activation = \"softmax\")\n\nsummary(model)## Model: \"sequential\"\n## ________________________________________________________________________________\n##  Layer (type)                       Output Shape                    Param #     \n## ================================================================================\n##  flatten (Flatten)                  (None, 11)                      0           \n##  dense_1 (Dense)                    (None, 128)                     1536        \n##  dense (Dense)                      (None, 7)                       903         \n## ================================================================================\n## Total params: 2,439\n## Trainable params: 2,439\n## Non-trainable params: 0\n## ________________________________________________________________________________"},{"path":"neural-network.html","id":"编译模型","chapter":"第 8 章 神经网络","heading":"8.2.2.1 编译模型","text":"在准备对模型进行训练之前，还需要再对其进行一些设置。以下内容是在模型的编译步骤中添加的：损失函数：用于测量模型在训练期间的准确率。您会希望最小化此函数，以便将模型”引导”到正确的方向上。优化器：决定模型如何根据其看到的数据和自身的损失函数进行更新。指标：用于监控训练和测试步骤。以下示例使用了准确率，即被正确分类的比率。","code":"\nmodel %>%\n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )"},{"path":"neural-network.html","id":"训练定类模型","chapter":"第 8 章 神经网络","heading":"8.2.2.2 训练定类模型","text":"训练神经网络模型需要执行以下步骤：将训练数据馈送给模型。在本例中，训练数据位于 wine_train 中。模型学习将 11 个自变量和 1 个因变量（品质）关联起来。要求模型对测试集（在本例中为 wine_test 数组）进行预测。验证预测是否与 wine_test 数组中的品质相匹配。","code":"\nmodel %>% \n  fit(\n    x = x_train, y = y_train.nom,\n    epochs = 10,\n    validation_split = 0.3,\n    verbose = 2\n  )"},{"path":"neural-network.html","id":"评估定类准确率","chapter":"第 8 章 神经网络","heading":"8.2.2.3 评估定类准确率","text":"接下来，比较模型在测试数据集上的表现：预测结果是自变量在 7 个因变量分类中的概率，使用 .max() 或者 k_argmax() 来获得预测的分类。","code":"\nmodel %>%\n  evaluate(\n    x = x_test,\n    y = y_test.nom,\n    verbose = 0\n  )##      loss  accuracy \n## 2.2769380 0.5466304\npredictions = predict(model, wine_test[,1:11] %>% as.matrix())\npredictions##                 [,1]         [,2]       [,3]       [,4]         [,5]\n##    [1,] 3.738237e-03 2.715563e-01 0.68221629 0.04217224 1.457058e-04\n##    [2,] 1.107448e-02 8.695627e-02 0.28778127 0.61161149 7.725548e-04\n##    [3,] 3.262162e-03 1.328468e-01 0.16614285 0.69758451 5.504251e-05\n##    [4,] 6.736268e-04 3.616212e-02 0.57422549 0.38891539 2.046533e-05\n##                 [,6]         [,7]\n##    [1,] 3.595377e-05 1.351962e-04\n##    [2,] 1.131449e-03 6.725920e-04\n##    [3,] 6.761569e-05 4.096814e-05\n##    [4,] 1.261270e-06 1.727117e-06\n##  [ reached getOption(\"max.print\") -- omitted 1465 rows ]\nclass.prediction = apply(predictions, 1, which.max)\ntable(wine_test$quality, class.prediction)##    class.prediction\n##       2   3   4\n##   0   0   3   3\n##   1   4  27  18\n##   2   1 292 144\n##   3   0 152 507\n##   4   0  12 252\n##   5   0   1  52\n##   6   0   0   1\nclass_pred = model %>% \n  predict(wine_test[,1:11] %>% as.matrix()) %>%\n  k_argmax()\nclass_pred[1:20]## tf.Tensor([2 3 3 2 2 3 2 3 2 3 2 3 3 2 3 2 2 1 1 2], shape=(20), dtype=int64)"},{"path":"neural-network.html","id":"训练定序模型","chapter":"第 8 章 神经网络","heading":"8.2.2.4 训练定序模型","text":"说明：模型在测试数据集上的准确率略低于训练数据集。训练准确率和测试准确率之间的差距代表过拟合。过拟合是指机器学习模型在新的、以前未曾见过的输入上的表现不如在训练数据上的表现。过拟合的模型会”记住”训练数据集中的噪声和细节，从而对模型在新数据上的表现产生负面影响。","code":"\nmodel %>% \n  fit(\n    x = x_train, y = y_train.ord,\n    epochs = 5,\n    validation_split = 0.3,\n    verbose = 2\n  )\n\nmodel %>%\n  evaluate(\n    x = x_test,\n    y = y_test.ord,\n    verbose = 0\n  )##       loss   accuracy \n## 18.9299660  0.3158611"},{"path":"neural-network.html","id":"保存预测模型","chapter":"第 8 章 神经网络","heading":"8.2.2.5 保存预测模型","text":"使用 save_model_tf() 保存神经网络模型，load_model_tf() 导入预先创建的神经网络模型。","code":"\nsave_model_tf(object = model, filepath = \"model\")\nreloaded_model = load_model_tf(\"model\")"},{"path":"neural-network.html","id":"使用经典的-rsnns-神经网络模型","chapter":"第 8 章 神经网络","heading":"8.2.3 使用经典的 RSNNS 神经网络模型","text":"RSNNS (Bergmeir 2021)是 R 到 SNNS 神经网络模拟器的接口，含有很多神经网络的常规程序。首先，尝试构建一个包含 3 个隐含层的神经网络，每层包含的神经元个数均为 5。调整神经网络的参数，可得到四种神经网络模型。第一种模型，将因变量看做定类变量，不使用权衰减；第二种模型，将因变量看做定序变量，不使用权衰减。","code":""},{"path":"neural-network.html","id":"定类预测模型","chapter":"第 8 章 神经网络","heading":"8.2.3.1 定类预测模型","text":"除了隐含层之外，神经网络还包含 1 个输入层和 1 个输出层。输出层输出 7 维向量，分别指代将预测归为 0 - 6 类的可能性。针对测试数据集，得出了一个预测矩阵。使用 .max() 可以计算得到预测分类。计算预测分类与实际分类相符的比例，可以得到预测准确率 accu。","code":"\nlibrary(RSNNS)\n\nsize1 = size2 = size3 = 5\n\n# 第一种模型\nmlp.nom.nodecay = mlp(\n  x_train, y_train.nom,\n  size = c(size1, size2, size3),\n  inputsTest = x_test,\n  targetsTest = y_test.nom,\n  maxit = 300,  # 迭代次数 300 次\n  learnFuncParams = c(0.1)   # 学习速率指定为 0.1\n)\nsummary(mlp.nom.nodecay)## SNNS network definition file V1.4-3D\n## generated at Fri Jul 22 16:34:22 2022\n## \n## network name : RSNNS_untitled\n## source files :\n## no. of units : 33\n## no. of connections : 140\n## no. of unit types : 0\n## no. of site types : 0\n## \n## \n## learning function : Std_Backpropagation\n## update function   : Topological_Order\n## \n## \n## unit default section :\n## \n## act      | bias     | st | subnet | layer | act func     | out func\n## ---------|----------|----|--------|-------|--------------|-------------\n##  0.00000 |  0.00000 | i  |      0 |     1 | Act_Logistic | Out_Identity \n## ---------|----------|----|--------|-------|--------------|-------------\n## \n## \n## unit definition section :\n## \n## no. | typeName | unitName                   | act      | bias     | st | position | act func     | out func | sites\n## ----|----------|----------------------------|----------|----------|----|----------|--------------|----------|-------\n##   1 |          | Input_fixed.acidity        |  2.66062 | -0.09042 | i  |  1, 0, 0 | Act_Identity |          | \n##   2 |          | Input_volatile.acidity     | -0.08176 | -0.04284 | i  |  2, 0, 0 | Act_Identity |          | \n##   3 |          | Input_citric.acid          |  0.95694 | -0.12396 | i  |  3, 0, 0 | Act_Identity |          | \n##   4 |          | Input_residual.sugar       |  0.82976 | -0.08442 | i  |  4, 0, 0 | Act_Identity |          | \n##   5 |          | Input_chlorides            | -0.49306 | -0.11317 | i  |  5, 0, 0 | Act_Identity |          | \n##   6 |          | Input_free.sulfur.dioxide  | -0.42971 | -0.14879 | i  |  6, 0, 0 | Act_Identity |          | \n##   7 |          | Input_total.sulfur.dioxide | -0.33791 |  0.01519 | i  |  7, 0, 0 | Act_Identity |          | \n##   8 |          | Input_density              |  0.99389 | -0.08476 | i  |  8, 0, 0 | Act_Identity |          | \n##   9 |          | Input_pH                   |  0.07770 | -0.07360 | i  |  9, 0, 0 | Act_Identity |          | \n##  10 |          | Input_sulphates            | -0.26153 |  0.05686 | i  | 10, 0, 0 | Act_Identity |          | \n##  11 |          | Input_alcohol              | -0.09285 |  0.06731 | i  | 11, 0, 0 | Act_Identity |          | \n##  12 |          | Hidden_2_1                 |  0.99696 |  5.02715 | h  |  1, 2, 0 |||\n##  13 |          | Hidden_2_2                 |  0.08200 |  5.90087 | h  |  2, 2, 0 |||\n##  14 |          | Hidden_2_3                 |  0.04845 | -4.57345 | h  |  3, 2, 0 |||\n##  15 |          | Hidden_2_4                 |  0.00225 | -2.70346 | h  |  4, 2, 0 |||\n##  16 |          | Hidden_2_5                 |  0.99960 |  2.87550 | h  |  5, 2, 0 |||\n##  17 |          | Hidden_3_1                 |  0.83918 | -2.32939 | h  |  1, 4, 0 |||\n##  18 |          | Hidden_3_2                 |  0.43934 | -0.70195 | h  |  2, 4, 0 |||\n##  19 |          | Hidden_3_3                 |  0.03598 | -0.96130 | h  |  3, 4, 0 |||\n##  20 |          | Hidden_3_4                 |  0.96628 |  4.12000 | h  |  4, 4, 0 |||\n##  21 |          | Hidden_3_5                 |  0.00110 | -1.21502 | h  |  5, 4, 0 |||\n##  22 |          | Hidden_4_1                 |  0.08368 | -0.61586 | h  |  1, 6, 0 |||\n##  23 |          | Hidden_4_2                 |  0.06837 | -1.25086 | h  |  2, 6, 0 |||\n##  24 |          | Hidden_4_3                 |  0.27048 | -0.34682 | h  |  3, 6, 0 |||\n##  25 |          | Hidden_4_4                 |  0.26613 |  0.05758 | h  |  4, 6, 0 |||\n##  26 |          | Hidden_4_5                 |  0.00094 | -2.64866 | h  |  5, 6, 0 |||\n##  27 |          | Output_1                   |  0.00789 | -3.68038 | o  |  1, 8, 0 |||\n##  28 |          | Output_2                   |  0.04981 | -2.19191 | o  |  2, 8, 0 |||\n##  29 |          | Output_3                   |  0.28387 | -1.77216 | o  |  3, 8, 0 |||\n##  30 |          | Output_4                   |  0.48984 |  0.29632 | o  |  4, 8, 0 |||\n##  31 |          | Output_5                   |  0.08363 | -2.13337 | o  |  5, 8, 0 |||\n##  32 |          | Output_6                   |  0.01890 | -2.97440 | o  |  6, 8, 0 |||\n##  33 |          | Output_7                   |  0.00397 | -4.18404 | o  |  7, 8, 0 |||\n## ----|----------|----------------------------|----------|----------|----|----------|--------------|----------|-------\n## \n## \n## connection definition section :\n## \n## target | site | source:weight\n## -------|------|---------------------------------------------------------------------------------------------------------------------\n##     12 |      | 11: 1.17533, 10:-2.29937,  9:-6.66261,  8: 1.46657,  7: 0.68845,  6:-3.28525,  5:-0.03179,  4:-0.56736,  3: 0.06485,\n##                  2:-1.39152,  1:-0.58841\n##     13 |      | 11: 0.32641, 10: 1.12937,  9: 0.72528,  8:-4.38482,  7: 4.67952,  6: 1.76739,  5:-3.38483,  4: 5.04722,  3:-0.91595,\n##                  2:-1.56693,  1:-2.42682\n##     14 |      | 11: 1.80964, 10: 0.00389,  9: 2.41577,  8:-4.92140,  7:-3.67977,  6: 2.95735,  5: 4.09392,  4: 2.93247,  3: 0.37301,\n##                  2: 2.22932,  1: 2.21998\n##     15 |      | 11:-3.60042, 10:-0.43310,  9: 1.94476,  8:-4.22507,  7:-0.27289,  6: 2.43773,  5: 3.16652,  4: 2.96072,  3: 0.93482,\n##                  2: 2.59101,  1:-0.15585\n##     16 |      | 11: 5.46517, 10:-2.28258,  9:-4.35267,  8:-0.45891,  7:-1.89646,  6: 1.28182,  5: 0.36506,  4: 2.27794,  3: 1.85913,\n##                  2:-4.38059,  1: 0.64088\n##     17 |      | 16: 7.05743, 15:-5.63959, 14: 7.18344, 13: 4.97279, 12:-3.82779\n##     18 |      | 16: 0.86002, 15:-1.74810, 14: 0.41966, 13:-1.20181, 12:-0.32039\n##     19 |      | 16: 0.18784, 15:-0.98449, 14: 1.68488, 13: 3.13555, 12:-2.85971\n##     20 |      | 16:-2.85886, 15: 7.86949, 14:-4.32676, 13:-5.82726, 12: 2.77120\n##     21 |      | 16:-2.68190, 15: 0.20840, 14: 3.89628, 13: 0.10669, 12:-3.12057\n##     22 |      | 21:-2.65286, 20: 0.40298, 19: 1.46786, 18:-1.14719, 17:-2.04097\n##     23 |      | 21:-1.78100, 20: 0.41633, 19: 0.29024, 18:-0.82707, 17:-1.67854\n##     24 |      | 21:-0.55685, 20: 5.94989, 19:-3.27249, 18:-0.03116, 17:-7.46270\n##     25 |      | 21:-3.07400, 20: 0.05806, 19: 0.41109, 18:-1.63107, 17:-0.50387\n##     26 |      | 21: 1.91336, 20:-7.78376, 19: 0.78595, 18:-0.06261, 17: 3.81588\n##     27 |      | 26:-1.79173, 25:-2.32646, 24:-1.14769, 23:-1.67660, 22:-1.28655\n##     28 |      | 26:-2.01496, 25:-4.43360, 24: 2.68938, 23:-1.44994, 22:-2.42583\n##     29 |      | 26:-1.43037, 25: 0.89108, 24: 2.41071, 23:-0.23925, 22:-0.29473\n##     30 |      | 26:-1.77575, 25: 0.95935, 24:-2.74178, 23: 0.61485, 22: 1.30207\n##     31 |      | 26: 2.32794, 25: 0.61271, 24:-0.82757, 23:-1.60088, 22:-1.10738\n##     32 |      | 26: 1.60963, 25:-0.87671, 24:-1.69464, 23:-2.34775, 22:-1.48526\n##     33 |      | 26:-1.71402, 25:-2.18103, 24:-1.84506, 23:-1.80076, 22:-1.63982\n## -------|------|---------------------------------------------------------------------------------------------------------------------\nmlp.nom.nodecay$fittedTestValues##                 [,1]        [,2]       [,3]      [,4]       [,5]        [,6]\n##    [1,] 0.0011958960 0.063806362 0.69769418 0.2011648 0.02870502 0.002036538\n##    [2,] 0.0026136232 0.004968469 0.06197268 0.3214716 0.44166523 0.106326461\n##    [3,] 0.0113041410 0.030774411 0.14375675 0.5848016 0.13624929 0.042650860\n##    [4,] 0.0011565066 0.035125636 0.65277147 0.2708599 0.03234187 0.002309690\n##                 [,7]\n##    [1,] 0.0003261598\n##    [2,] 0.0016948864\n##    [3,] 0.0069644679\n##    [4,] 0.0003383789\n##  [ reached getOption(\"max.print\") -- omitted 1465 rows ]\nclass.func.nom = function(prob){\n  which.max(prob) - 1\n}\n\nresult.nom = apply(mlp.nom.nodecay$fittedTestValues, 1, class.func.nom)\n\n# 预测准确率\naccu = sum(result.nom == wine_test$quality) / length(wine_test$quality)\naccu## [1] 0.5316542\ntable(wine_test$quality, result.nom)##    result.nom\n##       2   3   4\n##   0   2   3   1\n##   1  30  18   1\n##   2 257 162  18\n##   3 141 401 117\n##   4  13 128 123\n##   5   0  21  32\n##   6   0   1   0"},{"path":"neural-network.html","id":"定序预测模型","chapter":"第 8 章 神经网络","heading":"8.2.3.2 定序预测模型","text":"第二种模型，使用了定序自变量作为输出。定序预测向量需要经过处理，才能得到预测得出的类。计算按序数加权的分类准确率 weighted.accu。注意这个准确率是取所有预测分类平均值之后的结果。例如实际分类是 3，预测分类是 6，则这一预测的加权准确率为 0.5。通过改变隐含层的数量、每层包含的神经元数目以及指定权衰减参数，可以找出最佳的神经网络参数。","code":"\n# 第二种模型\nmlp.ord.nodecay = mlp(\n  x_train, y_train.ord,\n  size = c(size1, size2, size3),\n  inputsTest = x_test,\n  targetsTest = y_test.ord,\n  maxit = 300,  # 迭代次数 300 次\n  learnFuncParams = c(0.1)\n)\nsummary(mlp.ord.nodecay)## SNNS network definition file V1.4-3D\n## generated at Fri Jul 22 16:34:23 2022\n## \n## network name : RSNNS_untitled\n## source files :\n## no. of units : 33\n## no. of connections : 140\n## no. of unit types : 0\n## no. of site types : 0\n## \n## \n## learning function : Std_Backpropagation\n## update function   : Topological_Order\n## \n## \n## unit default section :\n## \n## act      | bias     | st | subnet | layer | act func     | out func\n## ---------|----------|----|--------|-------|--------------|-------------\n##  0.00000 |  0.00000 | i  |      0 |     1 | Act_Logistic | Out_Identity \n## ---------|----------|----|--------|-------|--------------|-------------\n## \n## \n## unit definition section :\n## \n## no. | typeName | unitName                   | act      | bias     | st | position | act func     | out func | sites\n## ----|----------|----------------------------|----------|----------|----|----------|--------------|----------|-------\n##   1 |          | Input_fixed.acidity        |  2.66062 | -0.04435 | i  |  1, 0, 0 | Act_Identity |          | \n##   2 |          | Input_volatile.acidity     | -0.08176 | -0.09033 | i  |  2, 0, 0 | Act_Identity |          | \n##   3 |          | Input_citric.acid          |  0.95694 | -0.07457 | i  |  3, 0, 0 | Act_Identity |          | \n##   4 |          | Input_residual.sugar       |  0.82976 |  0.22712 | i  |  4, 0, 0 | Act_Identity |          | \n##   5 |          | Input_chlorides            | -0.49306 |  0.29597 | i  |  5, 0, 0 | Act_Identity |          | \n##   6 |          | Input_free.sulfur.dioxide  | -0.42971 | -0.26006 | i  |  6, 0, 0 | Act_Identity |          | \n##   7 |          | Input_total.sulfur.dioxide | -0.33791 |  0.21530 | i  |  7, 0, 0 | Act_Identity |          | \n##   8 |          | Input_density              |  0.99389 | -0.03818 | i  |  8, 0, 0 | Act_Identity |          | \n##   9 |          | Input_pH                   |  0.07770 |  0.15574 | i  |  9, 0, 0 | Act_Identity |          | \n##  10 |          | Input_sulphates            | -0.26153 | -0.05758 | i  | 10, 0, 0 | Act_Identity |          | \n##  11 |          | Input_alcohol              | -0.09285 | -0.21438 | i  | 11, 0, 0 | Act_Identity |          | \n##  12 |          | Hidden_2_1                 |  0.34363 | -4.35920 | h  |  1, 2, 0 |||\n##  13 |          | Hidden_2_2                 |  0.08588 | -3.69903 | h  |  2, 2, 0 |||\n##  14 |          | Hidden_2_3                 |  0.12189 | -2.50810 | h  |  3, 2, 0 |||\n##  15 |          | Hidden_2_4                 |  0.08477 | -0.10556 | h  |  4, 2, 0 |||\n##  16 |          | Hidden_2_5                 |  0.00009 | -1.20430 | h  |  5, 2, 0 |||\n##  17 |          | Hidden_3_1                 |  0.93755 |  3.99283 | h  |  1, 4, 0 |||\n##  18 |          | Hidden_3_2                 |  0.27820 | -2.13244 | h  |  2, 4, 0 |||\n##  19 |          | Hidden_3_3                 |  0.47025 | -0.95064 | h  |  3, 4, 0 |||\n##  20 |          | Hidden_3_4                 |  0.91193 |  1.35685 | h  |  4, 4, 0 |||\n##  21 |          | Hidden_3_5                 |  0.12988 | -0.55366 | h  |  5, 4, 0 |||\n##  22 |          | Hidden_4_1                 |  0.29471 |  0.14253 | h  |  1, 6, 0 |||\n##  23 |          | Hidden_4_2                 |  0.13828 | -1.35980 | h  |  2, 6, 0 |||\n##  24 |          | Hidden_4_3                 |  0.41025 | -0.40076 | h  |  3, 6, 0 |||\n##  25 |          | Hidden_4_4                 |  0.04305 | -0.63027 | h  |  4, 6, 0 |||\n##  26 |          | Hidden_4_5                 |  0.25076 | -0.64585 | h  |  5, 6, 0 |||\n##  27 |          | Output_1                   |  0.99627 |  3.55066 | o  |  1, 8, 0 |||\n##  28 |          | Output_2                   |  0.99494 |  3.32781 | o  |  2, 8, 0 |||\n##  29 |          | Output_3                   |  0.99161 |  1.93115 | o  |  3, 8, 0 |||\n##  30 |          | Output_4                   |  0.90427 |  0.76531 | o  |  4, 8, 0 |||\n##  31 |          | Output_5                   |  0.23092 | -1.18909 | o  |  5, 8, 0 |||\n##  32 |          | Output_6                   |  0.03573 | -2.75134 | o  |  6, 8, 0 |||\n##  33 |          | Output_7                   |  0.00395 | -3.59696 | o  |  7, 8, 0 |||\n## ----|----------|----------------------------|----------|----------|----|----------|--------------|----------|-------\n## \n## \n## connection definition section :\n## \n## target | site | source:weight\n## -------|------|---------------------------------------------------------------------------------------------------------------------\n##     12 |      | 11: 0.02085, 10:-0.12125,  9: 0.16916,  8: 2.42068,  7: 0.16272,  6:-2.79721,  5:-0.33754,  4:-2.96055,  3: 0.04716,\n##                  2: 0.72049,  1: 0.90962\n##     13 |      | 11: 1.26876, 10:-1.37035,  9: 1.24834,  8:-1.60201,  7: 0.19984,  6: 4.02323,  5:-1.92827,  4: 3.28013,  3: 0.72335,\n##                  2: 4.24264,  1: 0.13800\n##     14 |      | 11:-2.45536, 10: 1.00573,  9: 2.35195,  8:-4.16185,  7: 1.44174,  6:-2.58316,  5:-0.68068,  4: 1.43230,  3:-1.37219,\n##                  2: 1.32214,  1: 1.42690\n##     15 |      | 11:-2.70611, 10:-0.29327,  9:-3.90370,  8: 1.83350,  7:-0.52748,  6: 2.04732,  5: 2.79783,  4:-0.89289,  3:-3.66339,\n##                  2: 0.34585,  1: 0.84013\n##     16 |      | 11:-0.31444, 10:-0.28840,  9: 0.13738,  8: 5.82739,  7: 1.24058,  6:-3.05437,  5: 0.51921,  4:-3.20950,  3: 0.82515,\n##                  2: 3.69858,  1:-4.70046\n##     17 |      | 16:-2.02493, 15:-0.08087, 14: 3.02753, 13:-2.42140, 12:-4.18441\n##     18 |      | 16: 0.85594, 15: 0.42083, 14: 0.86782, 13: 0.82347, 12: 2.81333\n##     19 |      | 16:-0.25699, 15: 0.19339, 14: 0.83815, 13:-1.18815, 12: 2.37169\n##     20 |      | 16: 2.07943, 15: 4.59461, 14:-2.97684, 13:-3.24768, 12: 3.58692\n##     21 |      | 16:-2.83498, 15: 1.04129, 14:-0.74130, 13:-3.51474, 12:-3.03864\n##     22 |      | 21: 2.04475, 20:-2.97757, 19:-0.58431, 18:-1.02946, 17: 2.12869\n##     23 |      | 21:-2.85933, 20: 4.16152, 19:-0.69126, 18: 0.49724, 17:-3.95366\n##     24 |      | 21:-1.98459, 20: 0.97153, 19:-2.16466, 18:-1.33866, 17: 0.85324\n##     25 |      | 21: 2.13270, 20:-3.81400, 19:-1.00606, 18:-2.22728, 17: 1.94414\n##     26 |      | 21: 0.32557, 20:-1.21032, 19:-0.69347, 18:-1.20205, 17: 1.35806\n##     27 |      | 26: 1.76955, 25: 1.33268, 24: 1.98136, 23: 1.77074, 22: 1.62223\n##     28 |      | 26: 1.74212, 25: 1.42696, 24: 1.85553, 23: 1.47047, 22: 1.66679\n##     29 |      | 26: 2.43411, 25: 2.11947, 24: 3.00497, 23:-1.03736, 22: 3.56425\n##     30 |      | 26: 1.44889, 25: 2.34874, 24: 1.72966, 23:-1.88682, 22: 1.92428\n##     31 |      | 26: 0.62692, 25: 2.57297, 24:-0.56995, 23:-2.68616, 22: 1.09680\n##     32 |      | 26:-0.08466, 25: 0.83976, 24:-1.08070, 23:-3.52710, 22: 1.26279\n##     33 |      | 26:-1.28895, 25:-1.23703, 24:-2.01185, 23:-1.55254, 22:-1.75126\n## -------|------|---------------------------------------------------------------------------------------------------------------------\nmlp.ord.nodecay$fittedTestValues##              [,1]      [,2]      [,3]      [,4]       [,5]        [,6]\n##    [1,] 0.9956080 0.9927572 0.8125862 0.3427341 0.02674903 0.002521048\n##    [2,] 0.9990558 0.9988108 0.9995784 0.9883357 0.62902498 0.107953534\n##    [3,] 0.9958932 0.9934549 0.8989763 0.4917446 0.04070766 0.004083691\n##    [4,] 0.9954665 0.9931853 0.9498710 0.6689847 0.07421679 0.008716500\n##                 [,7]\n##    [1,] 0.0051480541\n##    [2,] 0.0010526127\n##    [3,] 0.0047064577\n##    [4,] 0.0049733184\n##  [ reached getOption(\"max.print\") -- omitted 1465 rows ]\nclass.func.ord = function(prob){\n  if (any(prob > 0.5)){\n    return(max(which(prob > 0.5)) - 1)\n  } else {\n    return(NULL)\n  }\n}\n\nresult.ord = apply(mlp.ord.nodecay$fittedTestValues, 1, class.func.ord)\n\n# 按序数加权准确率\nweighted.accu = mean(1-abs(result.ord - wine_test$quality)/(6-0))\nweighted.accu## [1] 0.8990243\ntable(wine_test$quality, result.ord)##    result.ord\n##       2   3   4\n##   0   2   3   1\n##   1   7  42   0\n##   2  16 411  10\n##   3   5 575  79\n##   4   0 183  81\n##   5   0  28  25\n##   6   0   1   0"},{"path":"neural-network.html","id":"使用神经网络预测手机用户流失","chapter":"第 8 章 神经网络","heading":"8.3 使用神经网络预测手机用户流失","text":"","code":""},{"path":"neural-network.html","id":"读取测试数据","chapter":"第 8 章 神经网络","heading":"8.3.1 读取测试数据","text":"生成自变量矩阵。","code":"\nrequire(dplyr)\nrequire(readr)\n# 读取 10 个学习数据集\nlearn = vector(mode = \"list\", 10)\nfor (k in 1:10){\n  file = xfun::magic_path(paste0(\"ch3_mobile_learning_sample\", k, \"_imputed.csv\"))\n  learn[[k]] = read_csv(file, locale = locale(encoding = \"GB2312\")) %>%\n    arrange(`设备编码`)\n}\n\n# 读取 10 个测试数据集\ntest = lapply(1:10, function(k){\n  paste0(\"ch3_mobile_test_sample\", k, \"_imputed.csv\") %>%\n    xfun::magic_path() %>%\n    read_csv(locale = locale(encoding = \"GB2312\")) %>%\n    arrange(`设备编码`)\n})\nx_train = lapply(learn, function(x){\n  x[,-c(1,59)] %>% as.matrix() \n})\ny_train = lapply(learn, function(y) y[[59]])\n\nx_valid = lapply(test, function(x){\n  x[,-c(1,59)] %>% as.matrix()\n})\ny_valid = lapply(test, function(y) y[[59]])"},{"path":"neural-network.html","id":"初始化神经网络","chapter":"第 8 章 神经网络","heading":"8.3.2 初始化神经网络","text":"初始化一个包含 3 个隐含层的神经网络。","code":"\nrequire(keras)\nmodel = keras_model_sequential() %>%\n  layer_flatten(input_shape = 57) %>%\n  layer_dense(units = 100, activation = \"relu\")  %>%\n  # layer_dense(units = 30, activation = \"relu\") %>%\n  # layer_dense(units = 30, activation = \"relu\") %>%\n  layer_dense(units = 2, activation = \"softmax\")\n\nmodel %>% compile(\n  loss = 'sparse_categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = c(\"accuracy\")\n)"},{"path":"neural-network.html","id":"训练神经网络","chapter":"第 8 章 神经网络","heading":"8.3.3 训练神经网络","text":"这里，依次使用 10 个插值数据集对神经网络进行训练。训练后的分类准确度可以超过 90%。","code":"\nfor (i in 1:length(x_train)){\n  model %>% fit(\n    x_train[[i]],\n    y_train[[i]],\n    epochs = 25,\n    verbose = 1\n  )\n}"},{"path":"neural-network.html","id":"测试神经网络","chapter":"第 8 章 神经网络","heading":"8.3.4 测试神经网络","text":"测试神经网络在 10 个测试数据集上的表现，发现测试的分类准确度要低一些，仅有 90% 多。","code":"\nfor (i in 1:length(x_valid)){\n  model %>% evaluate(\n    x_valid[[i]],\n    y_valid[[i]],\n    verbose = 2\n  )\n}"},{"path":"neural-network.html","id":"预测结果","chapter":"第 8 章 神经网络","heading":"8.3.5 预测结果","text":"","code":"\nprediction = model %>%\n  predict(x_valid[[1]]) %>%\n  apply(MARGIN = 1, FUN = which.max)\n\ntable(y_valid[[1]], prediction-1)##    \n##        0    1\n##   0 5219  632\n##   1   24  132"},{"path":"neural-network.html","id":"tensorflow-神经网络的优化","chapter":"第 8 章 神经网络","heading":"8.3.6 TensorFlow 神经网络的优化","text":"在上面使用的 keras_model_sequential() 神经网络中，网络的层数（layer）、节点数（units）、训练时候（epoch）的设置都会影响网络的准确性。目前保留的参数，可以达到预测准确性超 93%。","code":""},{"path":"neural-network.html","id":"安装-tensorflow","chapter":"第 8 章 神经网络","heading":"8.4 安装 TensorFlow","text":"SNNS 是一个经典的神经网络模拟器，本书采用了 RSNNS 软件包来使用 SNNS 构建神经网络模型。不过，这个模拟器在已经不更新了。作者推荐使用 TensorFlow 或 PyTorch。TensorFlow 是一个端到端开源机器学习平台 借助 TensorFlow，初学者和专家可以轻松地创建机器学习模型。要安装 TensorFlow，则必须首先安装 Python/Conda，参见：https://docs.anaconda.com/anaconda/install/index.html。使用 tensorflow 软件包提供的 install_tensorflow() 函数可以快速安装 TensorFlow。这里将其安装到一个名为 “tensorflow”的 Conda 环境中。Keras 是一个用 Python 编写的高级神经网络 API，它能够以 TensorFlow, CNTK, 或者 Theano 作为后端运行。Keras 的开发重点是支持快速的实验。能够以最小的时延把你的想法转换为实验结果，是做好研究的关键。TensorFlow 的高阶 API 基于 Keras API 标准，用于定义和训练神经网络。Keras 通过用户友好的 API 实现快速原型设计、先进技术研究和生产。","code":"\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"tensorflow\") # 必须首先安装 Python/Conda\ninstall.packages(\"keras\")"},{"path":"neural-network.html","id":"安装-gpu-支持","chapter":"第 8 章 神经网络","heading":"8.4.1 安装 GPU 支持","text":"注意：如果仅学习的话，GPU 支持并非是必须的。要使用 GPU 加速，则必须在系统中安装以下 NVIDIA® 软件：NVIDIA® GPU 驱动程序 - CUDA® 11.2 要求 450.80.02 或更高版本。CUDA® 工具包：TensorFlow 支持 CUDA® 11.2（TensorFlow 2.5.0 及更高版本）CUDA® 工具包附带的 CUPTI。cuDNN SDK 8.1.0 cuDNN 版本。（可选）TensorRT 6.0，可缩短用某些模型进行推断的延迟时间并提高吞吐量。","code":""},{"path":"decision-tree.html","id":"decision-tree","chapter":"第 9 章 决策树","heading":"第 9 章 决策树","text":"决策树根据自变量的值进行递归划分以预测因变量。如果因变量是分类变量，则称为分类树；如果因变量是连续变量，则称为回归树。决策树有以下优点：可以直接处理定类自变量；无需考虑对定序或连续自变量的转换；对自变量的误差和异常值比较稳健，因为决策树只使用连续自变量取值的大小顺序；能够有效的处理缺失值；决策树的规则很容易解释；同时也有如下缺点：很难发现基于多个变量组合的规则；能达到局部最优，但是无法达到全局最优；树的结构很不稳定。","code":""},{"path":"officer.html","id":"officer","chapter":"第 10 章 处理 Office 文件","heading":"第 10 章 处理 Office 文件","text":"","code":""},{"path":"officer.html","id":"批量修改-ppt-的文件名","chapter":"第 10 章 处理 Office 文件","heading":"10.1 批量修改 PPT 的文件名","text":"read_pptx() 读取 *.pptx 文件，pptx_summary() 获取文件的基本信息。文件名取自第一章 PPT 中最长的中文字符串。为了判断字符的语言，可以使用多个软件包：textcat(R-textcat?) 软件包的 textcat() 函数。对中文支持很差。fastText(R-fastText?) 软件包的 language_identification() 函数。虽然支持中文，对中日字符的区分有问题，总是把中文识别为日文。cld3(R-cld3?) 软件包的 detect_language() 函数。R Wrapper Google’s Compact Language Detector 3，识别准确率最高。","code":"\nfiles = list.files(\"E:/资源/PPT 总汇/\",\"*.pptx\", full.names = TRUE,recursive = TRUE)\nlibrary(officer)\n\nid_name = stringr::str_extract(files, \"[^/\\\\.]*[0-9]{13}[^/\\\\.]*\")\nstudent_id = stringr::str_extract(id_name, \"[0-9]{13}\") # 学校\nstudent_name = stringr::str_remove(id_name, \"[0-9]{13}\") |> trimws() # 姓名\nppt_title = lapply(files, function(file){\n  d = read_pptx(file) |>\n    pptx_summary() |>\n    dplyr::filter(slide_id == 1, content_type == \"paragraph\") |>\n    dplyr::mutate(lang = cld3::detect_language(text)) |>\n    dplyr::filter(lang == \"zh\",\n                  !stringr::str_detect(text, \"汇报人|答辩人|华中农业大学|资源与环境学院|日期|[0-9]{13}|土壤学\")) |>\n    dplyr::pull(text) |>\n    trimws()\n  if (length(d) > 0) return(d[[1]])\n  return(basename(file) |> stringr::str_remove(\"\\\\.pptx\"))\n}) |>\n  unlist()\n\npath_to = \"E:/资源/PPT 总汇/\"\nsuccess = sapply(seq_along(ppt_title), function(i){\n  id = student_id[[i]]\n  name = student_name[[i]]\n  content = ppt_title[[i]] |> \n    stringr::str_remove(id) |>\n    stringr::str_remove(name) |>\n    trimws()\n  newname = paste(id, name, content, sep = \" \")\n  filename = xfun::with_ext(newname, \".pptx\")\n  to = file.path(path_to, filename)\n  file.rename(files[[i]], to)\n})"},{"path":"ggtree-as-phylo-hclust2.html","id":"ggtree-as-phylo-hclust2","chapter":"第 11 章 ggtree 代码解析","heading":"第 11 章 ggtree 代码解析","text":"ggtree 可以处理进化树及聚类等一系列相关的信息。","code":"\nlibrary(ape)\ndata(\"bird.orders\")\nhc = as.hclust(bird.orders)\nphylo = as.phylo.hclust(hc)\ndend = as.dendrogram(hc)\nplot(bird.orders, main = paste0(\"class = \", class(bird.orders)))\nplot(hc, main = paste0(\"class = \", class(hc)), cex = 1)\nplot(dend, main = paste0(\"class = \", class(dend)), horiz = TRUE)\nplot(phylo, main = paste0(\"class = \", class(phylo)))\nlibrary(ggplot2)\nlibrary(ggtree)\nggtree(bird.orders) + geom_tiplab() + xlim(NA, 35) + labs(title = paste0(\"class = \", class(bird.orders)))\nggtree(hc) + geom_tiplab() + xlim(NA, 35) + labs(title = paste0(\"class = \", class(hc)))\nggtree(dend) + geom_tiplab() + xlim(NA, 35) + labs(title = paste0(\"class = \", class(dend)))\nggtree(phylo) + geom_tiplab() + xlim(NA, 35) + labs(title = paste0(\"class = \", class(phylo)))"},{"path":"ggtree-as-phylo-hclust2.html","id":"新函数","chapter":"第 11 章 ggtree 代码解析","heading":"11.1 新函数","text":"","code":"\nphylo2 = ggtree:::as.phylo.hclust2(hc, hang = 1)\n\nplot(phylo, main = paste0(\"class = \", class(phylo)))\nplot(phylo2,  main = paste0(\"class = \", class(phylo2)))\nggtree(phylo) + geom_tiplab() + xlim(NA, 35) + labs(title = paste0(\"class = \", class(phylo)))\nggtree(phylo2) + geom_tiplab() + xlim(NA, 35) + labs(title = paste0(\"class = \", class(phylo2)))"},{"path":"ggtree-as-phylo-hclust2.html","id":"数据结构","chapter":"第 11 章 ggtree 代码解析","heading":"11.2 数据结构","text":"使用 str() 函数可以解析 R 语言中对象的结构。R 语言中复杂对象都是一个 List，我们可以看到这两个 List 均包含 4 个数值以及 2 个属性。不妨比较一下 List 中各个变量的数值是否相同。由此可见，只有对象中的 edge.length 的数值不同。其它的数值（包括属性）都是一样的。这里需要实现了解一下 edge 这个变量的含义。它是一个 22 × 2 的矩阵，数值均为整数。这个整数实际上是树上节点的编号。树状结构的节点有两类，一类是末端节点，另一类是相连的节点。第一类节点即是树的 tip，第二类节点是树的 internal nodes。该树状结构总共有 23 个 tip 和 22 个 internal nodes。进化树的本质也是图，把这些节点都视为图上的 node 的话，则每一行即定义了一个图上的边。这个图的拓扑结构与进化树是一样的。了解这些信息，是我们进一步理解进化树对象的数据操作的前提。","code":"\nstr(phylo)## List of 4\n##  $ edge       : int [1:44, 1:2] 24 25 29 29 25 30 39 39 30 24 ...\n##  $ edge.length: num [1:44] 2.1 4.1 21.8 21.8 3 ...\n##  $ tip.label  : chr [1:23] \"Struthioniformes\" \"Tinamiformes\" \"Craciformes\" \"Galliformes\" ...\n##  $ Nnode      : int 22\n##  - attr(*, \"class\")= chr \"phylo\"\n##  - attr(*, \"order\")= chr \"cladewise\"\nstr(phylo2)## List of 4\n##  $ edge       : int [1:44, 1:2] 24 25 29 29 25 30 39 39 30 24 ...\n##  $ edge.length: num [1:44] 4.2 8.2 1 1 6 ...\n##  $ tip.label  : chr [1:23] \"Struthioniformes\" \"Tinamiformes\" \"Craciformes\" \"Galliformes\" ...\n##  $ Nnode      : int 22\n##  - attr(*, \"class\")= chr \"phylo\"\n##  - attr(*, \"order\")= chr \"cladewise\"\nslots = names(phylo)\nfor (i in seq_along(slots)){\n  slot = slots[[i]]\n  comparison = identical(phylo[[slot]], phylo2[[slot]])\n  print(paste0(slot, \": \", comparison))\n}## [1] \"edge: TRUE\"\n## [1] \"edge.length: FALSE\"\n## [1] \"tip.label: TRUE\"\n## [1] \"Nnode: TRUE\"\nphylo## \n## Phylogenetic tree with 23 tips and 22 internal nodes.\n## \n## Tip labels:\n##   Struthioniformes, Tinamiformes, Craciformes, Galliformes, Anseriformes, Turniciformes, ...\n## \n## Rooted; includes branch lengths.\nlibrary(igraph)\ng = graph_from_data_frame(phylo$edge)\nplot(g, layout = layout_as_tree)"},{"path":"ggtree-as-phylo-hclust2.html","id":"源代码解析","chapter":"第 11 章 ggtree 代码解析","heading":"11.3 源代码解析","text":"从上面的结果来看，.phylo.hclust2() 做的事情就是修改了对象中 edge.length 的值。接下来我们看它是如何修改这个值的。.phylo.hclust() 的源代码如下：.phylo.hclust2() 的源代码如下（这里首先列出了函数中使用的另一个小函数）：","code":"\nas.phylo.hclust = function (x, ...){\n  N <- dim(x$merge)[1]\n  edge <- matrix(0L, 2 * N, 2)\n  edge.length <- numeric(2 * N)\n  node <- integer(N)\n  node[N] <- N + 2L\n  cur.nod <- N + 3L\n  j <- 1L\n  for (i in N:1) {\n    edge[j:(j + 1), 1] <- node[i]\n    for (l in 1:2) {\n      k <- j + l - 1L\n      y <- x$merge[i, l]\n      if (y > 0) {\n        edge[k, 2] <- node[y] <- cur.nod\n        cur.nod <- cur.nod + 1L\n        edge.length[k] <- x$height[i] - x$height[y]\n      }\n      else {\n        edge[k, 2] <- -y\n        edge.length[k] <- x$height[i]\n      }\n    }\n    j <- j + 2L\n  }\n  if (is.null(x$labels)) \n    x$labels <- as.character(1:(N + 1))\n  obj <- list(edge = edge, edge.length = edge.length/2, tip.label = x$labels, \n              Nnode = N)\n  class(obj) <- \"phylo\"\n  reorder(obj)\n}\nedge2vec <- function(tr) {\n  parent <- tr$edge[,1]\n  child <- tr$edge[,2]\n  \n  ## use lookup table\n  pvec <- integer(max(tr$edge))\n  pvec[child] <- parent\n  return(pvec)\n}\n\nas.phylo.hclust2 = function(x, hang = 0.1, ...){\n  h = x\n  tr = ape::as.phylo(x)\n  ev = edge2vec(tr)\n  \n  nodes <- integer(length(h$height))\n  for (i in seq_along(nodes)) {\n    j <- h$merge[i,]\n    if (any(j < 0)) {\n      j2 <- j[j < 0][1]\n      nodes[i] <- ev[abs(j2)]\n    } else {\n      nodes[i] <- ev[nodes[j[1]]]\n    }\n  }\n  \n  len <- numeric(max(tr$edge))\n  len[nodes] <- h$height\n  pn <- ev[nodes]\n  pn[pn == 0] <- treeio::rootnode(tr)\n  len[nodes] <- len[pn] - len[nodes]\n  len[1:Ntip(tr)] <- hang #max(h$height)/10\n  \n  tr$edge.length <- len[tr$edge[,2]]\n  return(tr)\n}"},{"path":"ggtree-as-phylo-hclust2.html","id":"其它相关对象的数据结构","chapter":"第 11 章 ggtree 代码解析","heading":"11.4 其它相关对象的数据结构","text":"phylo 前面已经介绍过，在此不再赘述。hclust 对象是一个含有 6 个变量的 List。dend 对象是一个多层嵌套的 dendrogram 对象（像俄罗斯套娃）。","code":"\nstr(bird.orders)## List of 4\n##  $ edge       : int [1:44, 1:2] 24 25 26 26 25 27 28 28 27 24 ...\n##  $ Nnode      : int 22\n##  $ tip.label  : chr [1:23] \"Struthioniformes\" \"Tinamiformes\" \"Craciformes\" \"Galliformes\" ...\n##  $ edge.length: num [1:44] 2.1 4.1 21.8 21.8 3 1.3 21.6 21.6 22.9 1 ...\n##  - attr(*, \"class\")= chr \"phylo\"\nstr(hc)## List of 6\n##  $ merge : int [1:22, 1:2] -21 -18 -20 -9 -16 3 -3 -1 5 -11 ...\n##  $ height: num [1:22] 40.2 40.8 41.6 41.6 42.6 43.2 43.2 43.6 43.8 44.2 ...\n##  $ order : int [1:23] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ labels: chr [1:23] \"Struthioniformes\" \"Tinamiformes\" \"Craciformes\" \"Galliformes\" ...\n##  $ call  : language as.hclust.phylo(x = bird.orders)\n##  $ method: chr \"unknown\"\n##  - attr(*, \"class\")= chr \"hclust\"\nstr(dend)## --[dendrogram w/ 2 branches and 23 members at h = 56]\n##   |--[dendrogram w/ 2 branches and 5 members at h = 51.8]\n##   |  |--[dendrogram w/ 2 branches and 2 members at h = 43.6]\n##   |  |  |--leaf \"Struthioniformes\" \n##   |  |  `--leaf \"Tinamiformes\" \n##   |  `--[dendrogram w/ 2 branches and 3 members at h = 45.8]\n##   |     |--[dendrogram w/ 2 branches and 2 members at h = 43.2]\n##   |     |  |--leaf \"Craciformes\" \n##   |     |  `--leaf \"Galliformes\" \n##   |     `--leaf \"Anseriformes\" \n##   `--[dendrogram w/ 2 branches and 18 members at h = 54]\n##      |--leaf \"Turniciformes\" \n##      `--[dendrogram w/ 2 branches and 17 members at h = 52.6]\n##         |--leaf \"Piciformes\" \n##         `--[dendrogram w/ 2 branches and 16 members at h = 50]\n##            |--[dendrogram w/ 2 branches and 5 members at h = 48.8]\n##            |  |--leaf \"Galbuliformes\" \n##            |  `--[dendrogram w/ 2 branches and 4 members at h = 46.8]\n##            |     |--[dendrogram w/ 2 branches and 2 members at h = 41.6]\n##            |     |  |--leaf \"Bucerotiformes\" \n##            |     |  `--leaf \"Upupiformes\" \n##            |     `--[dendrogram w/ 2 branches and 2 members at h = 44.2]\n##            |        |--leaf \"Trogoniformes\" \n##            |        `--leaf \"Coraciiformes\" \n##            `--[dendrogram w/ 2 branches and 11 members at h = 49]\n##               |--leaf \"Coliiformes\" \n##               `--[dendrogram w/ 2 branches and 10 members at h = 47.4]\n##                  |--leaf \"Cuculiformes\" \n##                  `--[dendrogram w/ 2 branches and 9 members at h = 46.2]\n##                     |--leaf \"Psittaciformes\" \n##                     `--[dendrogram w/ 2 branches and 8 members at h = 45]\n##                        |--[dendrogram w/ 2 branches and 4 members at h = 43.8]\n##                        |  |--[dendrogram w/ 2 branches and 2 members at h = 42.6]\n##                        |  |  |--leaf \"Apodiformes\" \n##                        |  |  `--leaf \"Trochiliformes\" \n##                        |  `--[dendrogram w/ 2 branches and 2 members at h = 40.8]\n##                        |     |--leaf \"Musophagiformes\" \n##                        |     `--leaf \"Strigiformes\" \n##                        `--[dendrogram w/ 2 branches and 4 members at h = 43.2]\n##                           |--[dendrogram w/ 2 branches and 3 members at h = 41.6]\n##                           |  |--leaf \"Columbiformes\" \n##                           |  `--[dendrogram w/ 2 branches and 2 members at h = 40.2]\n##                           |     |--leaf \"Gruiformes\" \n##                           |     `--leaf \"Ciconiiformes\" \n##                           `--leaf \"Passeriformes\""},{"path":"references.html","id":"references","chapter":"参考文献","heading":"参考文献","text":"","code":""}]

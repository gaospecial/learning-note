# 线性模型和广义线性模型 {#linear-model}

线性模型是实际应用中最常用的统计模型。

## 模型假设

### 线性模型假设

经典线性模型的应用是有前提条件的。自变量 $x = (x_1, ..., x_p)^T$ 被看做是给定的，而因变量 $Y$ 来自均值为 $μ$，方差为 $σ^2$ 的正态分布 $N(μ, σ^2)$，其中 $μ$ 与 自变量 $x$ 的关系是：

$$μ = \alpha + x^T\beta = \alpha + \beta_1x_1 + ... + \beta_px_p$$

这里回归系数 $\alpha$ 是截距项，回归系数 $\beta = (\beta_1, ..., \beta_p)^T$ 是对自变量的斜率。

因为线性模型的假设，所以在应用的时候需要进行模型诊断，才能决定适用性。模型诊断需要注意的地方包括：

- 标准化残差图
- 异常点
- 自相关性
- 多重共现性

### 广义线性模型假设

广义线性模型则推广了线性模型，适用于因变量是定类变量、定序变量等的情形。广义线性模型有 3 个成分，分别是**随机成分**、**系统成分**、**连接函数**。

## 线性模型预测房屋价格

`ch6_house.csv` 数据集记录了某地区 21613 座房屋的 10 个变量信息，我们据此可以建立线性模型预测房屋价格。

`house` 数据集的 10 个变量的定义如下：

- price：房屋价格
- bedrooms：卧室数目
- bathrooms：卫生间数目
- sqft_living：住房面积（平方英尺）
- sqft_lot：房基地面积
- floors：楼层数目
- condition：房屋整体状况的好坏，取值 1 - 5
- grade：房屋等级，取值 1 - 13
- sqft_above：除地下室以外的住房面积
- yr_built：房屋建成年份（最早 1900，最晚 2015）

```{r}
library(readr)
file = xfun::magic_path("ch6_house.csv")
house = read_csv(file)
house
```

房屋价格变量本身是不符合正态分布的，将其进行转换后符合正态分布。（为什么？）

(ref:figcap-house-price-dist) 转换前后房屋价格的直方图和 QQ 图

```{r house-price-dist, fig.cap="(ref:figcap-house-price-dist)"}
par(mfrow = c(2,2))
hist(house$price)
qqnorm(house$price)
hist(log(house$price))
qqnorm(log(house$price))
```

### 对房屋数据进行标准化

将 price、sqft_living 等变量均取对数。

同时，因为时间是无限增长的，在建模数据集中出现的时间不同于预测数据集中出现的时间，所以时间无法直接应用于建模。必须对时间变量进行转换。

```{r}
library(dplyr)
varname = c("price", "sqft_living", "sqft_lot", "sqft_above")
house = house %>%
  mutate(across(varname, log)) %>%
  rename_with(~ paste0("log_",.x), .cols = varname) %>%
  mutate(age = 2015 - yr_built) %>% # 对年份进行转换
  select(-yr_built)
```

将数据集随机划分为学习数据集和测试数据集。

```{r}
set.seed(12345)
id_learning = sample(1:nrow(house), round(0.7 * nrow(house)))
house_learning = house[id_learning,]
house_test = house[-id_learning,]
```

### 拟合线性模型

对学习数据集建立线性模型。使用 `summary()` 查看模型系数的估计、残差的统计量、R 方等信息。从输出结果看，模型各自变量的贡献均比较显著，R 方值为 0.64。

```{r}
fit.lm = lm(log_price ~ ., data = house_learning)
summary(fit.lm)
```

提取模型的系数估计值。

```{r}
coefficients(fit.lm)
```

提取模型的系数置信区间，`level = 0.95` 表示提取 95% 置信区间。

```{r}
confint(fit.lm, level = 0.95)
```

提取模型的因变量拟合值和残差。

```{r}
yhat = fitted(fit.lm)
resid = residuals(fit.lm)
```

### 线性模型的模型诊断

图 \@ref(fig:lm-plot) 中：

左上是残差对拟合值的散点图。图中黑线表明，除了拟合值比较小的少部分数据点，残差的平均值接近于零，因而满足线性假设；对于不同的拟合值，残差围绕平均值变化的范围相当，因而满足同方差假设。

右上是标准化残差的正态 QQ 图。可以看出残差大致符合正态分布，但也有少数异常值偏离较大。

左下是标准化残差绝对值的平方根对拟合值的散点图，也可用于更方便的检查同方差假设是否成立。在本例中，大部分的数据点拟合值落在 [12, 15] 区间，在这一区间同方差假设是成立的（黑线水平，且黑线上下点的变化范围接近）。

右下是各观测的 Cook 距离图。从中可见，学习数据中的 5710 号观测点是异常点。

(ref:figcap-lm-plot) 对线性模型进行诊断。

```{r lm-plot, fig.cap="(ref:figcap-lm-plot)"}
par(mfrow = c(2, 2),
    mar = c(2.5, 2.5, 1.5, 1.5))
plot(fit.lm, which = 1:4)
```

### 优化线性模型

将异常点去掉，重新拟合线性模型。异常值就没有那么大了（图 \@ref(fig:lm2-plot)）。

(ref:figcap-lm2-plot) 对去除 1 个异常值后的新线性模型进行诊断。

```{r lm2-plot, fig.cap="(ref:figcap-lm2-plot)"}
fit2.lm = lm(log_price ~ ., data = house_learning[-5710,])
par(mfrow = c(2, 2),
    mar = c(2.5, 2.5, 1.5, 1.5))
plot(fit2.lm, which = 1:4)
```

使用所得的线性模型对测试数据集进行预测，计算均方根误差，查看预测价格与实际价格的偏差。

```{r}
pred.lm = predict(fit2.lm, house_test)

# 均方根误差
rmse.lm = sqrt(mean((exp(pred.lm)-exp(house_test$log_price))^2))

# x 轴预测价格，y 轴实际价格
plot(exp(pred.lm), exp(house_test$log_price)) 
abline(a = 0, b = 1, col = "red")
```

## 逻辑回归预测是否患糖尿病

`ch6_diabetes.csv` 数据集记录了 768 位印第安女性的糖尿病患病资料。

- Pregnacies：怀孕次数
- Glucose：餐后 2 h 血糖
- BloodPressure：舒张压（mmHg）
- SkinThickness：肱三头肌皮褶厚度（mm）
- Insulin：餐后 2 h 的胰岛素水平
- BMI：体重指数
- DiabetesPredigreeFunction：糖尿病谱系功能
- Age：年龄
- Outcome：因变量，1 表示有糖尿病，0 表示不患病。

```{r}
file = xfun::magic_path("ch6_diabetes.csv")
diabetes = read_csv(file)
diabetes
```

### 分层拆分糖尿病患病数据集

**sampling**[@R-sampling] 包有各种抽样函数，可用于分层抽样将数据集分为学习数据集和测试数据集。这里使用 `strata()` 函数分层取 70% 的数据作为学习数据集。参数 `stratanames` 指定了分层变量的名字；参数 `size` 给出每层随机抽取的观测数；参数 `method = "srswor"` 说明在每层中使用无放回的简单随机抽样。 

`learning_sample$ID_unit` 给出了抽样得到的结果，可用于获取拆分后的数据集。

```{r}
set.seed(12345)
library(sampling)

diabetes = diabetes %>% # 分层抽样需要将分层变量排序后才能进行
  arrange(Outcome)

learning_sample = strata(diabetes, stratanames = ("Outcome"),
                         size = round(0.7 * table(diabetes$Outcome)),
                         method = "srswor")

# 学习数据集
diabetes_learning = diabetes[learning_sample$ID_unit,]

# 测试数据集
diabetes_test = diabetes[-learning_sample$ID_unit,]
```

### 模型拟合和验证

使用 `glm()` 拟合广义线性模型。`family = "binomial"` 指定了连接函数的类型（因变量分布为二项分布），从而得到一个 Logit 模型。

```{r}
fit.logit = glm(Outcome ~ ., data = diabetes_learning,
                family = "binomial")
summary(fit.logit)
```

将 Logit 模型应用于测试数据集对因变量进行预测。`type = "response"` 指定预测值为因变量取 1 的概率，并使用概率是否大于 0.5 为分界线，预测因变量类别为 1 或 0。 

```{r}
test.pred.logit = 1 * (predict(fit.logit, diabetes_test, type = "response") > 0.5)
```

查看因变量真实值域预测值的列联表。真实值为 0（未患病）的有 134 例被预测为 0，16 例被预测为 1；真实值为 1（患病）的有 36 例被预测为 0，44 例被预测为 1。整个模型的预测准确性堪忧。

```{r}
table(diabetes_test$Outcome, test.pred.logit)
```

使用 LASSO 算法[^about-lasso] 进行变量选择可以得到最佳模型。因为最佳模型与原模型的效果差别不大，所以不再赘述。

[^about-lasso]: 在统计学和机器学习中，Lasso 算法（Least Absolute Shrinkage and Selection Operator，又译最小绝对值收敛和选择算子、套索算法）是一种同时进行特征选择和正则化（数学）的回归分析方法，旨在增强统计模型的预测准确性和可解释性。

## 逻辑回归分析预测手机用户流失

本部分使用的移动运营商数据有多个数据集构成，分别记录了某移动运营商流失客户的信息和使用行为，以及未流失客户的信息和使用行为。

### 读取预处理好的数据集

在进行广义线性模型回归分析前，已经对数据进行了分层抽样，并将建模数据集分为学习数据集和测试数据集，并在学习数据集中通过**欠抽样**抽取了 10 个样本数据集 [^under-sampling]。

[^under-sampling]: 抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。过抽样（也叫上采样，over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本。欠抽样（也叫下采样，under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。（作者：宋天龙；链接：https://www.zhihu.com/question/66408862/answer/245813803）


```{r}
# 读取 10 个学习数据集
learn = vector(mode = "list", 10)
for (k in 1:10){
  file = xfun::magic_path(paste0("ch3_mobile_learning_sample", k, "_imputed.csv"))
  learn[[k]] = read_csv(file, locale = locale(encoding = "GB2312")) %>%
    arrange(`设备编码`)
}

# 读取 10 个测试数据集
test = lapply(1:10, function(k){
  paste0("ch3_mobile_test_sample", k, "_imputed.csv") %>%
    xfun::magic_path() %>%
    read_csv(locale = locale(encoding = "GB2312")) %>%
    arrange(`设备编码`)
})
```

### 建立（均值）逻辑回归模型

根据学习数据集的每个插补后样本数据集建立逻辑模型。使用去掉 `设备编码` 后的数据集为输入，以 `是否流失` 为因变量，其它变量为自变量，`family = "binomial"` 指定因变量分布为二项分布。

```{r}
library(glmnet)
fit.logit = lapply(1:10, function(k){
  # 逻辑回归模型
   glm( `是否流失` ~ ., 
                   data = learn[[k]] %>% select(-`设备编码`),
                   family = "binomial")
})
```

将根据学习数据集的 10 个插补后数据集分别建立模型所得的 10 组预测流失概率进行平均，得到测试数据集的预测流失概率。

```{r}
prob.logit.set = lapply(1:10, function(k){
  # 将模型应用于相应的测试数据集
  predict(fit.logit[[k]], test[[k]], type = "response")
})
prob.logit = bind_rows(prob.logit.set) %>% colMeans()
```

### 计算逻辑回归模型预测的准确率

生成列联表后，分别计算模型预测的准确率。

```{r}
class.logit = 1 * (prob.logit > 0.5)
conmat.logit = table(test[[1]][["是否流失"]], class.logit)
conmat.logit
```


```{r}
# 未流失用户被正确预测的比例
accu.y0.logit = conmat.logit[1, 1]/sum(conmat.logit[1, ])

# 流失用户被正确预测的比例
accu.y1.logit = conmat.logit[2, 2]/sum(conmat.logit[2, ])

# 所有用户被正确预测的比例
accu.logit = sum(diag(conmat.logit))/sum(conmat.logit) # 使用 diag() 取矩阵的对角线数值
```

## LASSO 分析预测手机用户流失

接下来使用 LASSO 分析进行手机用户流失预测，并比较 LASSO 分析得出的模型与逻辑回归模型的准确性。

**注意**：LASSO 本身并不等价于线性模型，而是在针对多个变量建模时，为了简化线性模型，用来帮助选择线性模型变量的方法。

### 使用学习数据集建立 LASSO 回归模型

使用 10 个运营商数据插补生成的学习数据集进行建模。`cv.glmnet()` 对 glmnet 模型进行多重交叉验证，然后返回一个 `cv.glmnet` 类。该类不仅包括预测模型，还包括交叉验证中计算得到的多个成分。

```{r}
cvfit.lasso = lapply(1:10, function(k){
  # 只使用学习数据集中的自变量矩阵
  x_learn = as.matrix(learn[[k]][, 2:58])
  
  # 使用交叉验证选出调节参数 lambda 的最佳值
  cv.glmnet(x_learn, learn[[k]][['是否流失']],
            family = "binomial",
            type.measure = "class")
})
```

将模型应用于相应的插补后测试数据集进行预测，并计算 LASSO 模型的回归系数。

```{r}
prob.lasso = lapply(1:10, function(k){
  x_test = as.matrix(test[[k]][, 2:58])
  predict(cvfit.lasso[[k]], x_test,
          s = 'lambda.min',
          type = "response")
})

coef.lasso = lapply(1:10, function(k){
  as.matrix(coef(cvfit.lasso[[k]], s = 'lambda.min'))[, 1]
})

```

### 计算 LASSO 模型的预测准确率

计算 10 个 LASSO 模型平均的预测流失概率，并以概率 0.5 为阈值计算模型的预测准确率。

```{r}
prob.lasso.mean = do.call("cbind", prob.lasso) %>%
  rowMeans()

class.lasso = 1 * (prob.lasso.mean > 0.5)
conmat.lasso = table(test[[1]][["是否流失"]], class.lasso)
conmat.lasso
```

分别计算不同类型的预测准确度。

```{r}
# 未流失用户被正确预测的比例
accu.y0.lasso = conmat.lasso[1, 1]/sum(conmat.lasso[1, ])

# 流失用户被正确预测的比例
accu.y1.lasso = conmat.lasso[2, 2]/sum(conmat.lasso[2, ])

# 所有用户被正确预测的比例
accu.lasso = sum(diag(conmat.lasso))/sum(conmat.lasso) # 使用 diag() 取矩阵的对角线数值
```

## 比较逻辑回归模型和 LASSO 模型

上面针对同样的运营商数据，分别建立了逻辑回归模型和 LASSO 模型，这两种模型得到的结果会有什么不同呢？接下来将回答这个问题。

### 模型预测准确率的差异

两种预测模型的准确率差异并不大。

```{r}
accu = tibble(
  type = c("未流失用户准确预测比例", "流失用户准确预测比例", "所有用户准确预测比例"),
  logit = c(accu.y0.logit, accu.y1.logit, accu.logit),
  lasso = c(accu.y0.lasso, accu.y1.lasso, accu.lasso)
)
accu
```

### 模型使用的自变量

根据方法的原理，可以得出 LASSO 模型使用的自变量可能会比逻辑模型要少一些。因此，下面分别分析每个 LASSO 模型纳入的自变量。

因为运营商数据包含的数据维度多达 57 个，所以在这些模型中实际上纳入的自变量可能是不同的，并且即便是两个模型纳入了同一个自变量，其在模型中的作用也可能是不同的。为了展示这种现象，这里将自变量系数区分为 3 类，大于 0，小于 0 和等于 0。

```{r}
coef.indic.lasso = lapply(1:10, function(k){
  coef.lasso[[k]][-1] %>%  # 去掉截距项
    t() %>%
    as_tibble()
}) %>%
  bind_rows() %>%
  mutate_all( .funs = function(x) ifelse(x == 0, 0, ifelse(x > 0, 1, -1)))
```

可以发现：在 10 个模型中，均为使用的变量就有 8 个，另外有其它变量被模型使用的次数在 1 - 9 之间，而被 10 个模型均纳入的变量只有 6 个。

```{r}
colSums(abs(coef.indic.lasso)) %>% table() %>%
  barplot()
```

超过 5 个模型选用的自变量有约 17 个，它们被不同模型使用的情况如下所示。

```{r}
library(pheatmap)
library(RColorBrewer)
idx = which(colSums(abs(coef.indic.lasso)) > 5)
pheatmap(t(coef.indic.lasso[, idx]), 
         show_rownames = TRUE, 
         show_colnames = FALSE, 
         color = colorRampPalette(rev(brewer.pal(n = 7, name = "RdBu")))(100),
         breaks = seq(-1, 1, length.out = 100))
```

实际上，即便不使用 LASSO 分析，在广义线性模型中也会报告有哪些自变量在模型中占有更高的权重。`summary()` 输出结果中的最后一列（`Pr(>|z|`）中，凡是小于 0.05 的都被特别标注了出来，这些便是模型中的主要参量。

```{r}
summary(fit.logit[[1]])
```





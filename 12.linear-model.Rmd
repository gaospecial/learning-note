# 线性模型和广义线性模型 {#linear-model}

线性模型是实际应用中最常用的统计模型。

## 模型假设

### 线性模型假设

经典线性模型的应用是有前提条件的。自变量 $x = (x_1, ..., x_p)^T$ 被看做是给定的，而因变量 $Y$ 来自均值为 $μ$，方差为 $σ^2$ 的正态分布 $N(μ, σ^2)$，其中 $μ$ 与 自变量 $x$ 的关系是：

$$μ = \alpha + x^T\beta = \alpha + \beta_1x_1 + ... + \beta_px_p$$

这里回归系数 $\alpha$ 是截距项，回归系数 $\beta = (\beta_1, ..., \beta_p)^T$ 是对自变量的斜率。

因为线性模型的假设，所以在应用的时候需要进行模型诊断，才能决定适用性。模型诊断需要注意的地方包括：

- 标准化残差图
- 异常点
- 自相关性
- 多重共现性

### 广义线性模型假设

广义线性模型则推广了线性模型，适用于因变量是定类变量、定序变量等的情形。广义线性模型有 3 个成分，分别是**随机成分**、**系统成分**、**连接函数**。

## 线性模型预测房屋价格

`ch6_house.csv` 数据集记录了某地区 21613 座房屋的 10 个变量信息，我们据此可以建立线性模型预测房屋价格。

`house` 数据集的 10 个变量的定义如下：

- price：房屋价格
- bedrooms：卧室数目
- bathrooms：卫生间数目
- sqft_living：住房面积（平方英尺）
- sqft_lot：房基地面积
- floors：楼层数目
- condition：房屋整体状况的好坏，取值 1 - 5
- grade：房屋等级，取值 1 - 13
- sqft_above：除地下室以外的住房面积
- yr_built：房屋建成年份（最早 1900，最晚 2015）

```{r}
library(readr)
file = xfun::magic_path("ch6_house.csv")
house = read_csv(file)
house
```

房屋价格变量本身是不符合正态分布的，将其进行转换后符合正态分布。（为什么？）

(ref:figcap-house-price-dist) 转换前后房屋价格的直方图和 QQ 图

```{r house-price-dist, fig.cap="(ref:figcap-house-price-dist)"}
par(mfrow = c(2,2))
hist(house$price)
qqnorm(house$price)
hist(log(house$price))
qqnorm(log(house$price))
```

### 数据准备

将 price、sqft_living 等变量均取对数。

同时，因为时间是无限增长的，在建模数据集中出现的时间不同于预测数据集中出现的时间，所以时间无法直接应用于建模。必须对时间变量进行转换。

```{r}
library(dplyr)
varname = c("price", "sqft_living", "sqft_lot", "sqft_above")
house = house %>%
  mutate(across(varname, log)) %>%
  rename_with(~ paste0("log_",.x), .cols = varname) %>%
  mutate(age = 2015 - yr_built) %>% # 对年份进行转换
  select(-yr_built)
```

将数据集随机划分为学习数据集和测试数据集。

```{r}
set.seed(12345)
id_learning = sample(1:nrow(house), round(0.7 * nrow(house)))
house_learning = house[id_learning,]
house_test = house[-id_learning,]
```

### 拟合线性模型

对学习数据集建立线性模型。使用 `summary()` 查看模型系数的估计、残差的统计量、R 方等信息。从输出结果看，模型各自变量的贡献均比较显著，R 方值为 0.64。

```{r}
fit.lm = lm(log_price ~ ., data = house_learning)
summary(fit.lm)
```

提取模型的系数估计值。

```{r}
coefficients(fit.lm)
```

提取模型的系数置信区间，`level = 0.95` 表示提取 95% 置信区间。

```{r}
confint(fit.lm, level = 0.95)
```

提取模型的因变量拟合值和残差。

```{r}
yhat = fitted(fit.lm)
resid = residuals(fit.lm)
```

### 模型诊断

图 \@ref(fig:lm-plot) 中：

左上是残差对拟合值的散点图。图中黑线表明，除了拟合值比较小的少部分数据点，残差的平均值接近于零，因而满足线性假设；对于不同的拟合值，残差围绕平均值变化的范围相当，因而满足同方差假设。

右上是标准化残差的正态 QQ 图。可以看出残差大致符合正态分布，但也有少数异常值偏离较大。

左下是标准化残差绝对值的平方根对拟合值的散点图，也可用于更方便的检查同方差假设是否成立。在本例中，大部分的数据点拟合值落在 [12, 15] 区间，在这一区间同方差假设是成立的（黑线水平，且黑线上下点的变化范围接近）。

右下是各观测的 Cook 距离图。从中可见，学习数据中的 5710 号观测点是异常点。

(ref:figcap-lm-plot) 对线性模型进行诊断。

```{r lm-plot, fig.cap="(ref:figcap-lm-plot)"}
par(mfrow = c(2, 2),
    mar = c(2.5, 2.5, 1.5, 1.5))
plot(fit.lm, which = 1:4)
```

### 模型优化

将异常点去掉，重新拟合线性模型。异常值就没有那么大了（图 \@ref(fig:lm2-plot)）。

(ref:figcap-lm2-plot) 对去除 1 个异常值后的新线性模型进行诊断。

```{r lm2-plot, fig.cap="(ref:figcap-lm2-plot)"}
fit2.lm = lm(log_price ~ ., data = house_learning[-5710,])
par(mfrow = c(2, 2),
    mar = c(2.5, 2.5, 1.5, 1.5))
plot(fit2.lm, which = 1:4)
```

使用所得的线性模型对测试数据集进行预测，计算均方根误差，查看预测价格与实际价格的偏差。

```{r}
pred.lm = predict(fit2.lm, house_test)

# 均方根误差
rmse.lm = sqrt(mean((exp(pred.lm)-exp(house_test$log_price))^2))

# x 轴预测价格，y 轴实际价格
plot(exp(pred.lm), exp(house_test$log_price)) 
abline(a = 0, b = 1, col = "red")
```

## 逻辑回归预测是否患糖尿病

`ch6_diabetes.csv` 数据集记录了 768 位印第安女性的糖尿病患病资料。

- Pregnacies：怀孕次数
- Glucose：餐后 2 h 血糖
- BloodPressure：舒张压（mmHg）
- SkinThickness：肱三头肌皮褶厚度（mm）
- Insulin：餐后 2 h 的胰岛素水平
- BMI：体重指数
- DiabetesPredigreeFunction：糖尿病谱系功能
- Age：年龄
- Outcome：因变量，1 表示有糖尿病，0 表示不患病。

```{r}
file = xfun::magic_path("ch6_diabetes.csv")
diabetes = read_csv(file)
diabetes
```

### 拆分数据集

**sampling**[@R-sampling] 包有各种抽样函数，可用于分层抽样将数据集分为学习数据集和测试数据集。这里使用 `strata()` 函数分层取 70% 的数据作为学习数据集。参数 `stratanames` 指定了分层变量的名字；参数 `size` 给出每层随机抽取的观测数；参数 `method = "srswor"` 说明在每层中使用无放回的简单随机抽样。 

`learning_sample$ID_unit` 给出了抽样得到的结果，可用于获取拆分后的数据集。

```{r}
set.seed(12345)
library(sampling)

diabetes = diabetes %>% # 分层抽样需要将分层变量排序后才能进行
  arrange(Outcome)

learning_sample = strata(diabetes, stratanames = ("Outcome"),
                         size = round(0.7 * table(diabetes$Outcome)),
                         method = "srswor")

# 学习数据集
diabetes_learning = diabetes[learning_sample$ID_unit,]

# 测试数据集
diabetes_test = diabetes[-learning_sample$ID_unit,]
```

### 模型拟合和验证

使用 `glm()` 拟合广义线性模型。`family = "binomial"` 指定了连接函数的类型（因变量分布为二项分布），从而得到一个 Logit 模型。

```{r}
fit.logit = glm(Outcome ~ ., data = diabetes_learning,
                family = "binomial")
summary(fit.logit)
```

将 Logit 模型应用于测试数据集对因变量进行预测。`type = "response"` 指定预测值为因变量取 1 的概率，并使用概率是否大于 0.5 为分界线，预测因变量类别为 1 或 0。 

```{r}
test.pred.logit = 1 * (predict(fit.logit, diabetes_test, type = "response") > 0.5)
```

查看因变量真实值域预测值的列联表。真实值为 0 的有 134 例被预测为 0，16 例被预测为 1；真实值为 1 的有 36 例被预测为 0，44 例被预测为 1。整个模型的预测准确性堪忧。

```{r}
table(diabetes_test$Outcome, test.pred.logit)
```

使用 LASSO 算法[^about-lasso] 进行变量选择可以得到最佳模型。因为最佳模型与原模型的效果差别不大，所以不再赘述。

[^about-lasso]: 在统计学和机器学习中，Lasso 算法（Least Absolute Shrinkage and Selection Operator，又译最小绝对值收敛和选择算子、套索算法）是一种同时进行特征选择和正则化（数学）的回归分析方法，旨在增强统计模型的预测准确性和可解释性。
